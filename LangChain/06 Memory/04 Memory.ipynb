{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23613b90-0ab0-49c3-bd35-0f2b3dc06655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain_community\n",
    "# !pip install langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ed523-d178-4f35-a56e-205c663ebf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is persisting?\n",
    "    # persisting means saving data so that it can be reused later.\n",
    "\n",
    "# What Does \"Persisting Memory\" Mean?\n",
    "    # When you persist memory, you're storing it in a permanent or semi-permanent storage like:\n",
    "        # - A database (e.g., PostgreSQL, Redis)\n",
    "        # - A file (e.g., JSON, CSV)\n",
    "        # - A cloud service (e.g., Pinecone, Zep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8dbd33-6c08-4bfb-8b00-6d0840560cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Memory module is responsible for persisting the state between calls of a chain or agent, which helps the language model remember \n",
    "# previous interactions and use that information to make better decisions.\n",
    "\n",
    "# It provides a standard interface for persisting state between calls of a chain or agent, enabling the language model to have memory and context.\n",
    "\n",
    "# Memory is critical for personal assistants, autonomous agents, and agent simulations, where the language model needs to remember previous \n",
    "# interactions to make informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cf610-76ce-4281-b8d5-7bde9c28c18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36cf13-9742-4d1e-aa1a-004903711f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does it do?\n",
    "    # The Memory module enables the language model to have memory and context, allowing the LLM to make informed decisions.\n",
    "    \n",
    "    # It allows the model to remember user inputs, system responses, and any other relevant information. \n",
    "    # The stored information can be accessed and utilized during subsequent interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344be49-1a9e-40db-bdaa-6a29b62681b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do I need it?\n",
    "    # The Memory module helps you build more interactive and personalized applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbeaf0-71ac-46bf-87db-cd69240a109c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973780af-6610-4beb-8e93-16f463414dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Why Memory Is Needed in GenAI Applications\n",
    "\n",
    "#  1.  `Maintaining Context`\n",
    "        # -   Memory allows the AI to  `remember previous messages`  in a conversation.\n",
    "        # -   This is crucial for  `multi-turn interactions`, like chatbots or virtual assistants..\n",
    "\n",
    "#  2.  `Personalization`\n",
    "        # -   Memory helps GenAI apps  `remember user preferences`, names, goals, or past behavior.\n",
    "\n",
    "#  3.  `Long-Term Knowledge`\n",
    "        # -   Some applications need to  `recall facts or decisions`  made in earlier sessions\n",
    "\n",
    "#  4.  `Efficient Reasoning`\n",
    "        # -   Memory can store  `intermediate steps or summaries`, helping the AI reason better over time.\n",
    "\n",
    "\n",
    "#  5.  `Semantic Recall`\n",
    "        # -   With vector-based memory, GenAI can  `recall similar past conversations`  even if phrased differently.\n",
    "\n",
    "#  6.  `Persistence Across Sessions`\n",
    "        # -   Memory enables  `continuity`  so users don’t have to repeat themselves every time they return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3577c-968b-4f89-8fc5-fe63485cf211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e0f96-acea-4255-8523-b47004a1fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Storage types\n",
    "#  - Permanent Storage\n",
    "#  - Temporary Storage\n",
    "\n",
    "# Permanent Storage Types\n",
    "    # 1. Databases\n",
    "    #     - PostgreSQL, MySQL, MongoDB, Redis (Structured data, chat history, user profiles)\n",
    "    # 2. File Systems\t\n",
    "    #     - JSON, CSV, TXT, XML files\t(Logs, configs, lightweight memory)\n",
    "    # 3. Vector Stores\t\n",
    "    #     - FAISS, Pinecone, Chroma\t (Embedding-based memory, semantic search)\n",
    "    # 4. Cloud Storage\t\n",
    "    #     - AWS S3, Google Cloud Storage, Azure Blob\t(Large files, backups, logs)\n",
    "    # 5. Key-Value Stores\t\n",
    "    #     - Redis, DynamoDB\t (Fast access to session or user data)\n",
    "    # 6. Memory Services\t\n",
    "    #    - Zep, LangChainHub\t(Specialized memory for LLMs and agents)\n",
    "\n",
    "\n",
    "# Temporary Storage Types\n",
    "    # 1. In-Memory Variables\t\n",
    "    #     - Python dictionaries, lists, objects\n",
    "    # 2. Session Memory\t\n",
    "    #     - ConversationBufferMemory, ConversationEntityMemory (without persistence)\n",
    "    # 3. Cache\t\n",
    "    #     - RAM-based cache, browser cache\t\n",
    "    # 4. Temporary Files\t\n",
    "    #     - OS temp folders, tempfile in Python\t\n",
    "    # 5. Ephemeral Databases\t\n",
    "    #     - In-memory SQLite, Redis (without persistence)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132c7f8-a6fc-4eb8-abdb-d6d69c30f349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736dfc5-e811-4d2d-81de-9817825b7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Types of memory in LangChain\n",
    "    # 1.  `ConversationBufferMemory`  – Stores the full conversation history in memory.\n",
    "    # 2.  `ConversationBufferWindowMemory`  – Keeps only the last  _k_  messages in memory.\n",
    "    # 3.  `ConversationSummaryMemory`  – Summarizes the conversation using an LLM to reduce memory size.\n",
    "    # 4.  `ConversationTokenBufferMemory`  – Maintains memory based on token count rather than message count.\n",
    "    # 5.  `ConversationEntityMemory` - Extracts named entities from the conversation \n",
    "    # 5.  `VectorStoreRetrieverMemory`  – Uses a vector store to retrieve relevant past interactions based on similarity.\n",
    "    # 6.  `ZepMemory`  – Integrates with Zep for long-term memory storage and retrieval.\n",
    "    # 7.  `PostgresChatMessageHistory`  /  `RedisChatMessageHistory`  – Stores memory in external databases like PostgreSQL or Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97643e2b-bc46-4bb1-a87e-e25ebf00d733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f711bd9-8b97-4361-8249-6b41c0ca6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What is Buffer?\n",
    "    # a buffer is a temporary storage area used to hold data. \n",
    "    # Why Use a Buffer?\n",
    "        # To maintain context across multiple turns in a conversation.\n",
    "        # To allow the model to generate more coherent and relevant responses by remembering past inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94996cc2-1302-4655-843b-6eda1abad972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### LangChain Memory Types & Usefulness\n",
    "\n",
    "# -   `ConversationBufferMemory`  \n",
    "#     Stores the full conversation history.  \n",
    "#     ➤ Useful for short to medium conversations where full context is needed.\n",
    "    \n",
    "# -   `ConversationBufferWindowMemory`  \n",
    "#     Keeps only the last  _k_  messages.  \n",
    "#     ➤ Useful for long conversations where only recent context matters.\n",
    "    \n",
    "# -   `ConversationSummaryMemory`  \n",
    "#     Summarizes the conversation using an LLM.  \n",
    "#     ➤ Useful for long conversations to retain key points without exceeding token limits.\n",
    "    \n",
    "# -   `ConversationTokenBufferMemory`  \n",
    "#     Stores messages based on token count.  \n",
    "#     ➤ Useful when working with models that have strict token limits.\n",
    "    \n",
    "# -   `VectorStoreRetrieverMemory`  \n",
    "#     Retrieves relevant past interactions using vector similarity.  \n",
    "#     ➤ Useful for semantic memory and long-term knowledge retrieval.\n",
    "    \n",
    "# -   `ZepMemory`  \n",
    "#     Integrates with Zep for persistent memory and search.  \n",
    "#     ➤ Useful for scalable, long-term memory across sessions.\n",
    "    \n",
    "# -   `PostgresChatMessageHistory`  /  `RedisChatMessageHistory`  \n",
    "#     Stores chat history in external databases.  \n",
    "#     ➤ Useful for persistent memory across users or sessions.\n",
    "\n",
    "# - `ConversationEntityMemory `\n",
    "#  - memory that tracks entities (like people, places, or things) mentioned during a conversation and stores information about them.\n",
    "#  - Extracts named entities from the conversation.\n",
    "#  - ➤ Useful Great for personal assistants, customer support bots, or multi-turn agents where tracking people, products, or topics is important.\n",
    "\n",
    "# - `VectorStoreRetrieverMemory`\n",
    "#  - It Enables semantic memory using embeddings and vector similarity search.\n",
    "#  - Instead of storing raw text, it stores vector representations of past interactions and retrieves the most relevant ones based on \n",
    "#    similarity to the current input.\n",
    "#  - ➤ Useful for \n",
    "# \t- Chatbots that need to remember facts across sessions.\n",
    "# \t- Assistants that need to recall similar past queries.\n",
    "# \t- Applications where semantic similarity is more important than exact match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fe461-6f6c-49dc-8f2b-de41d98795de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a1238-3861-4bb8-89a3-0522a27fa85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Temporary Memory Types\n",
    "\n",
    "    # These exist  only during the session  and are lost when the app stops or restarts (unless explicitly persisted).\n",
    "    \n",
    "    # -   `ConversationBufferMemory`\n",
    "    # -   `ConversationBufferWindowMemory`\n",
    "    # -   `ConversationSummaryMemory`\n",
    "    # -   `ConversationTokenBufferMemory`\n",
    "    # -   `ConversationEntityMemory`\n",
    "    # -   `VectorStoreRetrieverMemory`  (if using in-memory vector store like FAISS without saving)\n",
    "\n",
    "# ### Permanent Memory Types\n",
    "\n",
    "    # These are  persisted  to external storage and survive across sessions.\n",
    "    \n",
    "    # -   `PostgresChatMessageHistory`\n",
    "    # -   `RedisChatMessageHistory`\n",
    "    # -   `ZepMemory`\n",
    "    # -   `VectorStoreRetrieverMemory`  (when using persistent vector stores like Pinecone, Chroma with disk storage, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e23b6-d4af-4ef7-98c2-43d6efa365f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74e598-d1a7-4ddf-9c2e-7d9709c600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which memory type should you use?\n",
    "    #  - Short, exact recall: `ConversationBufferMemory`\n",
    "    #  - Long conversations: `ConversationSummaryMemory / ConversationSummaryBufferMemory`\n",
    "    #  - Short-term memory efficiency: `ConversationBufferWindowMemory`\n",
    "    #  - Entity-focused structured recall: `ConversationEntityMemory`\n",
    "    #  - Deep, semantic retrieval/connections: `VectorStoreRetrieverMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc60e8f-71f4-4619-8ba2-ae9d6d3a09eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601b170-5708-450c-a823-65935f9a5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Example Use Cases\n",
    "\n",
    "# -   **Chatbot for customer support**  →  `ConversationEntityMemory`  +  `VectorStoreRetrieverMemory`\n",
    "# -   **Personal assistant**  →  `ConversationSummaryMemory`  +  `ConversationEntityMemory`\n",
    "# -   **FAQ bot with long-term recall**  →  `VectorStoreRetrieverMemory`\n",
    "# -   **Multi-user app with login sessions**  →  `RedisChatMessageHistory`  or  `PostgresChatMessageHistory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56b3a3-d7bc-470d-8db2-3b5ea7672ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99223b-c34a-4e17-a88c-21b596919bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Choosing the Right Memory Type for Your Use Case\n",
    "\n",
    "# Selecting the appropriate memory type depends on several factors:\n",
    "\n",
    "# -   `Duration and Complexity`: Short sessions benefit from detailed context retention with ConversationBufferMemory, while \n",
    "#      long-term interactions may require summarization via ConversationSummaryMemory.\n",
    "# -   `Detail vs. Overview`: Determine whether detailed interaction history or high-level summaries are more valuable for your application.\n",
    "# -   `Performance`: Consider the trade-offs between the memory size and retrieval speed.\n",
    "\n",
    "# `Use Cases:`\n",
    "\n",
    "# -   `ConversationBufferMemory`: Ideal for quick customer support or FAQ-style interactions.\n",
    "# -   `ConversationSummaryMemory`: Best suited for long-term engagements like project management or ongoing customer interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21922db1-4392-4c68-8638-465d65742bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8e33c5bd-be67-4b6f-b742-b3ba27c70ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common issues/problems \n",
    "\n",
    "# Memory Bloat: Happens when too much conversation history is stored.\n",
    "    # ➤ Solution: Limit memory by number of messages (windowed) or token count.\n",
    "\n",
    "# Context Drift: The model loses track of the topic or entities.\n",
    "    # ➤ Solution: Use summarization or track entities explicitly.\n",
    "\n",
    "# High Token Costs: Large memory increases API costs.\n",
    "    # ➤ Solution: Keep memory compact and summarized.\n",
    "\n",
    "# Privacy Concerns: Storing user data can raise privacy issues.\n",
    "    # ➤ Solution: Use temporary (session-based) memory and define clear data policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ff449-52db-430a-805b-855febd0dbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ba04b-f2ab-4158-8332-0235153e0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `Memory bloat`  refers to the problem of storing  `too much conversation history or context`, which can lead to:\n",
    "\n",
    "# -   `High token usage`  (increased cost and slower responses)\n",
    "# -   `Reduced model performance`  (due to exceeding token limits)\n",
    "# -   `Unnecessary or irrelevant context`  being included in prompts\n",
    "\n",
    "# ### Why It Happens:\n",
    "\n",
    "# -   Using memory types that store  `entire conversation history`  (like  `ConversationBufferMemory`) without limits.\n",
    "# -   Not summarizing or trimming old messages.\n",
    "# -   Storing repeated or redundant information.\n",
    "# - \n",
    "# ### How to Avoid It:\n",
    "\n",
    "# -   Use  `windowed memory`  (`ConversationBufferWindowMemory`) to keep only the last  _k_  messages.\n",
    "# -   Use  `token-limited memory`  (`ConversationTokenBufferMemory`) to control memory size by token count.\n",
    "# -   Use  `summarization`  (`ConversationSummaryMemory`) to condense older parts of the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93825dc-66a8-4355-b02d-7dfac087b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c18dd3-138b-4acd-b4f2-4ed7126a41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `Context drift` happens when a generative AI model `loses track of the topic or entities` being discussed over the course of a conversation. \n",
    "#  This can lead to `irrelevant, confusing, or incorrect responses`.\n",
    "\n",
    "# ### Why It Happens:\n",
    "\n",
    "# -   The conversation becomes  `too long`, and earlier context is forgotten or pushed out.\n",
    "# -   The model  `misinterprets references`  like “he,” “it,” or “that” due to missing background.\n",
    "# -   The memory is  `not structured`  to track key entities or topics.\n",
    "\n",
    "# ### Example of Context Drift:\n",
    "\n",
    "# `User`: \"Tell me about Alice. She’s a software engineer.\"  \n",
    "# `Later`: \"Where does she work?\"  \n",
    "# → If the model forgets who “she” is, it may give a wrong or generic answer.\n",
    "\n",
    "\n",
    "# ###`Strategy`  How to Prevent It:\n",
    "#  1. `Summarize past context`\n",
    "#          - `ConversationSummaryMemory`\n",
    "#  2. `Track entities explicitly`\n",
    "#          - `ConversationEntityMemory`\n",
    "#  3. `Limit memory to recent turns`\n",
    "#          - `ConversationBufferWindowMemory`\n",
    "#  4. `Use semantic memory`\n",
    "#          - VectorStoreRetrieverMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be28ad4-d325-4cf1-9c33-8fb87fef057f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b5c4a-d667-43da-a9db-e569cb5d031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# `High token cost` refers to the `increased computational and financial cost` of using large numbers of tokens (words, symbols, or characters) \n",
    "#  in prompts and responses when interacting with language models like GPT.\n",
    "\n",
    "# ### Why It Happens:\n",
    "\n",
    "# -   Storing and sending  `long conversation histories`  to the model.\n",
    "# -   Using  `verbose prompts`  or  `unoptimized memory`.\n",
    "# -   Including  `irrelevant or redundant context`  in each request.\n",
    "\n",
    "# ### Why It Matters:\n",
    "\n",
    "# -   Most LLM APIs (like OpenAI, Anthropic)  `charge per token`.\n",
    "# -   More tokens =  `higher cost`  and  `slower response times`.\n",
    "# -   Can hit  `token limits`, causing errors or truncated outputs.\n",
    "\n",
    "# ### How to Reduce Token Costs:\n",
    "\n",
    "# -   `Summarize memory`: Use  `ConversationSummaryMemory`  to condense older messages.\n",
    "# -   `Limit memory size`: Use  `ConversationBufferWindowMemory`  or  `ConversationTokenBufferMemory`.\n",
    "# -   `Trim unnecessary context`: Only include relevant parts of the conversation.\n",
    "# -   `Use semantic memory`: Retrieve only the most relevant past messages using  `VectorStoreRetrieverMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c24a7-6c1c-45dd-8cd6-4638fc51c5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226b2b7-6cef-4e53-a439-06cf2377fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### `Benefits of Optimizing Memory in LLMs`\n",
    "\n",
    "# -   ` Reduced Token Costs`  \n",
    "#     By trimming unnecessary context and summarizing conversations, you reduce the number of tokens sent to the model—saving money and API usage.\n",
    "    \n",
    "# -   `  Faster Response Times`  \n",
    "#     Smaller prompts mean quicker processing, leading to faster replies and a smoother user experience.\n",
    "    \n",
    "# -   ` Improved Relevance and Accuracy`  \n",
    "#     Optimized memory helps the model focus on the most important and recent context, reducing confusion and improving the quality of responses.\n",
    "    \n",
    "# -   ` Better Token Management`  \n",
    "#     Helps stay within the model’s token limits, avoiding errors like truncated outputs or context loss.\n",
    "    \n",
    "# -   ` Enhanced Privacy Control`  \n",
    "#     By storing only essential or summarized data, you reduce the risk of exposing sensitive user information.\n",
    "    \n",
    "# -   ` Scalability`  \n",
    "#     Efficient memory handling allows your application to support more users and longer sessions without performance degradation.\n",
    "    \n",
    "# -   ` Easier Debugging and Maintenance`  \n",
    "#     Cleaner, more structured memory makes it easier to understand and debug the model’s behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c127e9-28bd-4c8c-a872-c287e8261ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd78fd-595f-4654-99bd-f62bf697b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### `Strategies to Optimize Memory in LLMs`\n",
    "\n",
    "# #### 1.  `Use Windowed or Token-Limited Memory`\n",
    "\n",
    "# -   Limit memory to the  `last few messages`  (`ConversationBufferWindowMemory`) or a  `maximum token count`  (`ConversationTokenBufferMemory`).\n",
    "# -   Prevents memory bloat and keeps prompts within model limits.\n",
    "\n",
    "# #### 2.  `Summarize Older Context`\n",
    "\n",
    "# -   Use  `ConversationSummaryMemory`  to  `compress earlier parts`  of the conversation into a summary.\n",
    "# -   Keeps the essence of the conversation while reducing token usage.\n",
    "\n",
    "# #### 3.  `Track Entities Instead of Full History`\n",
    "\n",
    "# -   Use  `ConversationEntityMemory`  to  `store facts about key entities`  (like people or places) instead of full messages.\n",
    "# -   Helps maintain relevance without storing everything.\n",
    "\n",
    "# #### 4.  `Use Semantic Memory for Relevance`\n",
    "\n",
    "# -   Implement  `VectorStoreRetrieverMemory`  to  `retrieve only relevant past interactions`  using embeddings.\n",
    "# -   Avoids sending irrelevant history to the model.\n",
    "\n",
    "# #### 5.  `Clear or Reset Memory When Needed`\n",
    "\n",
    "# -   For privacy or performance,  `reset memory`  at logical points (e.g., end of session, task completion).\n",
    "\n",
    "# #### 6.  `Avoid Redundant Information`\n",
    "\n",
    "# -   Don’t repeat static instructions or facts in every prompt.\n",
    "# -   Use system prompts or separate context injection when needed.\n",
    "\n",
    "# #### 7.  `Persist Only What Matters`\n",
    "\n",
    "# -   Store only  `important facts or summaries`  in persistent memory (e.g., Redis, Postgres, Pinecone).\n",
    "# -   Avoid saving full chat logs unless necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4c770-6538-4921-9951-d52817243796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8150e5d-94e4-49b2-8513-896a4181da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "from utils import (\n",
    "            get_model_openAI,\n",
    "            get_model_groq,\n",
    "            get_openAI_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_openAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b695e2f3-513c-4fab-b769-b49029bb1a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI and I'm here to assist you. How can I help you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hi, what's up?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc753cf-059b-4b6e-8d55-7be50840b8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f62d6f2b-a5bf-436d-a347-372477952840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Santhosh Kumar\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Santhosh Kumar! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Utilizing the Conversational Buffer Memory\n",
    "memory= ConversationBufferMemory()\n",
    "conversation=ConversationChain(memory=memory, \n",
    "                  llm=model,\n",
    "                  verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi, my name is Santhosh Kumar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40db3298-9cc7-45e9-b6b7-23ac86ee2775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Santhosh Kumar\n",
      "AI: Hello Santhosh Kumar! It's nice to meet you. How can I assist you today?\n",
      "Human: How many emotional states are there?\n",
      "AI: Emotions are complex and can be categorized in many ways. However, most psychologists agree on a set of six basic emotions that are universally experienced in all human cultures. These are: happiness, sadness, fear, anger, surprise, and disgust. Some researchers have expanded this list to include others like anticipation, trust, and love. However, it's important to remember that emotions can be mixed and nuanced, creating a vast array of emotional states.\n",
      "Human: How many emotional states are there?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As mentioned before, there are six basic emotions that are universally recognized: happiness, sadness, fear, anger, surprise, and disgust. However, this is a simplified model and some researchers argue for a more complex range of emotions. For instance, Robert Plutchik proposed an \"emotion wheel\" that includes 8 primary emotions and a variety of secondary and tertiary emotions, resulting in a total of 32 emotional states. In reality, the number can be even larger when you consider nuanced and mixed emotions. So, the exact number of emotional states can be quite vast and hard to quantify.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"How many emotional states are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca98f537-70c5-446d-a05b-f36aa26af278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Santhosh Kumar\n",
      "AI: Hello Santhosh Kumar! It's nice to meet you. How can I assist you today?\n",
      "Human: How many emotional states are there?\n",
      "AI: Emotions are complex and can be categorized in many ways. However, most psychologists agree on a set of six basic emotions that are universally experienced in all human cultures. These are: happiness, sadness, fear, anger, surprise, and disgust. Some researchers have expanded this list to include others like anticipation, trust, and love. However, it's important to remember that emotions can be mixed and nuanced, creating a vast array of emotional states.\n",
      "Human: How many emotional states are there?\n",
      "AI: As mentioned before, there are six basic emotions that are universally recognized: happiness, sadness, fear, anger, surprise, and disgust. However, this is a simplified model and some researchers argue for a more complex range of emotions. For instance, Robert Plutchik proposed an \"emotion wheel\" that includes 8 primary emotions and a variety of secondary and tertiary emotions, resulting in a total of 32 emotional states. In reality, the number can be even larger when you consider nuanced and mixed emotions. So, the exact number of emotional states can be quite vast and hard to quantify.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Santhosh Kumar, as you introduced yourself earlier in the conversation.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6542d0e-fcb6-41e7-b820-dbae1cd53628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2ea7c846-ec1a-4cd4-8b78-14d9f04654b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "## Adding COnversational in Memory as an example\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=model, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e837325-7d4d-4b88-a146-60a1cf329307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as an AI, I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I am designed to respect user privacy and confidentiality.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e05ad6-bc5d-4507-9e8f-a57c3f2da1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c4e5df0b-5215-40d5-a4dc-f825211460d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human greets the AI casually and the AI responds in kind. The conversation continues in a laid-back manner. When asked about the schedule for the day, the AI informs the human of a morning meeting with the product team, time set aside for work on the LangChain project, and a lunch meeting at an Italian restaurant with a customer interested in AI. The AI stresses the importance of being prepared with a PowerPoint presentation and a laptop for the LLM demo.'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "\n",
    "memory= ConversationSummaryMemory(llm=model, max_token_limit=100)\n",
    "\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c3689234-e088-4a4f-b98a-e389a413b753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI casually and the AI responds in kind. The conversation continues in a laid-back manner. When asked about the schedule for the day, the AI informs the human of a morning meeting with the product team, time set aside for work on the LangChain project, and a lunch meeting at an Italian restaurant with a customer interested in AI. The AI stresses the importance of being prepared with a PowerPoint presentation and a laptop for the LLM demo.\n",
      "Human: What cool demo in laptop can he show\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The LLM (Language Learning Machine) demo that he can show is really impressive. It's a machine learning model that can translate languages in real time. It can also analyze and understand natural language, making it capable of tasks like summarizing articles, answering questions, and even generating human-like text. It's a great illustration of the capabilities of AI and machine learning.\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation=ConversationChain(memory=memory, \n",
    "                  llm=model,\n",
    "                  verbose=True)\n",
    "\n",
    "conversation.predict(input=\"What cool demo in laptop can he show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6d30a-762c-442c-82d4-1017aad12e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "447f87f2-9118-48ee-aaa2-fe3eda804ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ConversationTokenBufferMemory\n",
    "\n",
    "# nstead of “k” conversations being remembered in ConversationBufferWindowMemory, \n",
    "# in this case we want to remember last set of discussion based on “max token limit”.\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory=ConversationTokenBufferMemory(llm=model, max_token_limit=25)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1fbef537-8bd8-4925-910f-91f02c8994ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09df0a-8f25-45d7-9d84-3ab7c1d318f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77069b07-02b9-4e74-9dbb-d1e133be173f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f924081-243d-4665-86c2-e436dabff72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain.memory import ConversationSummaryMemory  # Deprecated\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc957480-c360-4191-8d70-ec00d32c0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(llm=model, return_messages=True)\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"Santhosh and MayaManish are working on a hackathon project.\"},\n",
    "    {\"output\": \"That's awesome! What's the hackathon project about?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6025bd53-a7a4-4dab-bf33-3fd76ef4c0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Santhosh and MayaManish are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e33cedce-a6fb-458a-9634-7db06268d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary=memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3da8ab13-94f5-4f26-8301-db56b754fd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santhosh and MayaManish are working on a hackathon project.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(summary[\"history\"][0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f69650e-61a8-4929-ba93-22fb85c0d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"It's a machine learning project focused on healthcare.\"},\n",
    "    {\"output\": \"Sounds exciting! Are they building a prediction model or something else\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Yes, they’re working on a model to predict patient outcomes.\"},\n",
    "    {\"output\": \"Impressive! How far along are they with the project?\"}\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80e9016b-6d08-4f99-9c94-fcf838051069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Santhosh and MayaManish are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a543ca61-40f9-409a-8ad7-2613cde0ff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santhosh and MayaManish are working on a hackathon project.\n"
     ]
    }
   ],
   "source": [
    "summary=memory.load_memory_variables({})\n",
    "\n",
    "print(summary[\"history\"][0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3dc04b3-2a71-4a33-8933-e909a263bd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Santhosh and MayaManish are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe6fc286-15df-436d-bba8-5855e5a2889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print('Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42a97f64-ca07-453e-a618-6408a3a43ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Santhosh and MayaManish are working on a hackathon project.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}), AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}), AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc79b30d-f4dc-467d-964f-52236efabb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Santhosh and MayaManish are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53c18005-d074-4dae-9287-2448ba1a98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "previous_summary=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "798506f0-97f4-4b99-956b-9012752137f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Santhosh and MayaManish are working on a hackathon project.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's awesome! What's the hackathon project about?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"It's a machine learning project focused on healthcare.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sounds exciting! Are they building a prediction model or something else', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Yes, they’re working on a model to predict patient outcomes.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Impressive! How far along are they with the project?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.chat_memory.messages\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cd0e6a7-8508-43d0-9df4-6506045299a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConversationBufferMemory' object has no attribute 'predict_new_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_new_summary\u001b[49m(messages, previous_summary)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydantic\\main.py:994\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ConversationBufferMemory' object has no attribute 'predict_new_summary'"
     ]
    }
   ],
   "source": [
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1566d73-b3bc-4832-896f-e56fe371548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationSummaryMemory(human_prefix='User', llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001382CC53110>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001382CC5B4D0>, root_client=<openai.OpenAI object at 0x000001382B52AAD0>, root_async_client=<openai.AsyncOpenAI object at 0x000001382CC53250>, model_name='gpt-4', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********')), chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, how can I assist you today?', additional_kwargs={}, response_metadata={})]), buffer='The user greets the AI and the AI responds by asking how it can assist the user today.', memory_key='summary')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hi\")\n",
    "\n",
    "history.add_ai_message(\"Hello, how can I assist you today?\")\n",
    "\n",
    "ConversationSummaryMemory.from_messages(\n",
    "    llm=model,\n",
    "    chat_memory=history,\n",
    "    memory_key=\"summary\",\n",
    "    human_prefix=\"User\",\n",
    "    ai_prefix=\"AI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0354a1-2438-4e0f-9bc0-cfa2db267661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9551b11-efb2-4b3b-b1ca-c47c01c61d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm=model,\n",
    "    chat_memory=history,\n",
    "    memory_key=\"summary\",\n",
    "    human_prefix=\"User\",\n",
    "    ai_prefix=\"AI\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb98db1e-efed-4ff9-aa8e-c11331ab69f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The user greets the AI and the AI responds by asking how it can assist the user today.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5571d7-ead1-40ac-9eee-c36ab240ad7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b54a0840-4366-45af-9709-c1a2f3af7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From here the chaining starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "366a5ae6-ab51-44c1-8ef7-6e520c9d5442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm here, processing information and assisting users like you. How can I help you today?\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=ConversationBufferMemory(llm=model),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbfa30aa-ec01-4e2b-a7c5-434409a4c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI: Hello! I'm here, processing information and assisting users like you. How can I help you today?\n",
      "Human: Sunny and Mayank are working on a mlops production ready project.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's interesting! MLOps, or DevOps for machine learning, is all about bridging the gap between the development (Dev) and operation (Ops) sides of machine learning. It's a practice for collaboration and communication between data scientists and operations professionals to help manage production ML lifecycle. \\n\\nSunny and Mayank must be involved in tasks like building, testing, and deploying machine learning models efficiently and reliably. They may also be working on automating various steps in ML lifecycle such as data collection, model training, model serving, monitoring, etc. \\n\\nWould you like to know more about MLOps or details related to their project?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Sunny and Mayank are working on a mlops production ready project.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96013fb7-0f63-41fe-9087-9f28d09cf179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI: Hello! I'm here, processing information and assisting users like you. How can I help you today?\n",
      "Human: Sunny and Mayank are working on a mlops production ready project.\n",
      "AI: That's interesting! MLOps, or DevOps for machine learning, is all about bridging the gap between the development (Dev) and operation (Ops) sides of machine learning. It's a practice for collaboration and communication between data scientists and operations professionals to help manage production ML lifecycle. \n",
      "\n",
      "Sunny and Mayank must be involved in tasks like building, testing, and deploying machine learning models efficiently and reliably. They may also be working on automating various steps in ML lifecycle such as data collection, model training, model serving, monitoring, etc. \n",
      "\n",
      "Would you like to know more about MLOps or details related to their project?\n",
      "Human: so can you describe mlops pipeline to me with in six point.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Absolutely, here is a simplified description of an MLOps pipeline broken down into six points:\\n\\n1. **Data Collection and Preparation**: This is the first step where raw data is collected from various sources, cleaned, and transformed into a format that can be used by machine learning models.\\n\\n2. **Model Development and Training**: In this stage, data scientists use the prepared data to develop and train machine learning models. They experiment with different algorithms and tune parameters to find the most effective model.\\n\\n3. **Model Validation**: Once a model is trained, it is validated using a different set of data to assess its performance and accuracy. This can help identify any issues or biases in the model.\\n\\n4. **Model Deployment**: After validation, the model is deployed into a production environment. This might involve integrating the model into existing systems and setting up APIs for other applications to use.\\n\\n5. **Model Monitoring and Management**: After deployment, the model's performance is continuously monitored to ensure it is still providing accurate predictions. If the model’s performance degrades, it may need to be re-trained with new data.\\n\\n6. **Retraining and Updating Models**: The models might need to be retrained and updated over time as new data becomes available or if the performance of the model starts to decline. This should be an automated process to ensure the model is always performing at its best.\\n\\nIt's worth noting that MLOps aims to automate as many of these steps as possible to increase efficiency and reduce errors.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"so can you describe mlops pipeline to me with in six point.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c26be2b7-b4fd-4e4e-be2b-08ba68c1d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI: Hello! I'm here, processing information and assisting users like you. How can I help you today?\n",
      "Human: Sunny and Mayank are working on a mlops production ready project.\n",
      "AI: That's interesting! MLOps, or DevOps for machine learning, is all about bridging the gap between the development (Dev) and operation (Ops) sides of machine learning. It's a practice for collaboration and communication between data scientists and operations professionals to help manage production ML lifecycle. \n",
      "\n",
      "Sunny and Mayank must be involved in tasks like building, testing, and deploying machine learning models efficiently and reliably. They may also be working on automating various steps in ML lifecycle such as data collection, model training, model serving, monitoring, etc. \n",
      "\n",
      "Would you like to know more about MLOps or details related to their project?\n",
      "Human: so can you describe mlops pipeline to me with in six point.\n",
      "AI: Absolutely, here is a simplified description of an MLOps pipeline broken down into six points:\n",
      "\n",
      "1. **Data Collection and Preparation**: This is the first step where raw data is collected from various sources, cleaned, and transformed into a format that can be used by machine learning models.\n",
      "\n",
      "2. **Model Development and Training**: In this stage, data scientists use the prepared data to develop and train machine learning models. They experiment with different algorithms and tune parameters to find the most effective model.\n",
      "\n",
      "3. **Model Validation**: Once a model is trained, it is validated using a different set of data to assess its performance and accuracy. This can help identify any issues or biases in the model.\n",
      "\n",
      "4. **Model Deployment**: After validation, the model is deployed into a production environment. This might involve integrating the model into existing systems and setting up APIs for other applications to use.\n",
      "\n",
      "5. **Model Monitoring and Management**: After deployment, the model's performance is continuously monitored to ensure it is still providing accurate predictions. If the model’s performance degrades, it may need to be re-trained with new data.\n",
      "\n",
      "6. **Retraining and Updating Models**: The models might need to be retrained and updated over time as new data becomes available or if the performance of the model starts to decline. This should be an automated process to ensure the model is always performing at its best.\n",
      "\n",
      "It's worth noting that MLOps aims to automate as many of these steps as possible to increase efficiency and reduce errors.\n",
      "Human: How many total points are there?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the description I provided, there are six main points or stages in the MLOps pipeline. However, depending on the complexity of the project and the specific needs of a team, there might be more detailed steps or sub-steps within each of these main stages. In general, the six stages I mentioned cover the essential elements of an MLOps pipeline.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"How many total points are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5eb8f0b6-ebce-4da9-8d0c-ae7cd52cfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI: Hello! I'm here, processing information and assisting users like you. How can I help you today?\n",
      "Human: Sunny and Mayank are working on a mlops production ready project.\n",
      "AI: That's interesting! MLOps, or DevOps for machine learning, is all about bridging the gap between the development (Dev) and operation (Ops) sides of machine learning. It's a practice for collaboration and communication between data scientists and operations professionals to help manage production ML lifecycle. \n",
      "\n",
      "Sunny and Mayank must be involved in tasks like building, testing, and deploying machine learning models efficiently and reliably. They may also be working on automating various steps in ML lifecycle such as data collection, model training, model serving, monitoring, etc. \n",
      "\n",
      "Would you like to know more about MLOps or details related to their project?\n",
      "Human: so can you describe mlops pipeline to me with in six point.\n",
      "AI: Absolutely, here is a simplified description of an MLOps pipeline broken down into six points:\n",
      "\n",
      "1. **Data Collection and Preparation**: This is the first step where raw data is collected from various sources, cleaned, and transformed into a format that can be used by machine learning models.\n",
      "\n",
      "2. **Model Development and Training**: In this stage, data scientists use the prepared data to develop and train machine learning models. They experiment with different algorithms and tune parameters to find the most effective model.\n",
      "\n",
      "3. **Model Validation**: Once a model is trained, it is validated using a different set of data to assess its performance and accuracy. This can help identify any issues or biases in the model.\n",
      "\n",
      "4. **Model Deployment**: After validation, the model is deployed into a production environment. This might involve integrating the model into existing systems and setting up APIs for other applications to use.\n",
      "\n",
      "5. **Model Monitoring and Management**: After deployment, the model's performance is continuously monitored to ensure it is still providing accurate predictions. If the model’s performance degrades, it may need to be re-trained with new data.\n",
      "\n",
      "6. **Retraining and Updating Models**: The models might need to be retrained and updated over time as new data becomes available or if the performance of the model starts to decline. This should be an automated process to ensure the model is always performing at its best.\n",
      "\n",
      "It's worth noting that MLOps aims to automate as many of these steps as possible to increase efficiency and reduce errors.\n",
      "Human: How many total points are there?\n",
      "AI: In the description I provided, there are six main points or stages in the MLOps pipeline. However, depending on the complexity of the project and the specific needs of a team, there might be more detailed steps or sub-steps within each of these main stages. In general, the six stages I mentioned cover the essential elements of an MLOps pipeline.\n",
      "Human: can you give me 5th point with explaination\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, the 5th point in the MLOps pipeline is \"Model Monitoring and Management\". \\n\\nAfter a machine learning model is deployed into a production environment, it doesn\\'t mean the work is done. It\\'s crucial to continuously monitor the model\\'s performance to ensure it is still providing accurate and reliable predictions. This is what we mean by \"Model Monitoring\".\\n\\nThe performance of a model might degrade over time due to various factors, including changes in the underlying data it was trained on, also known as concept drift. This is why ongoing monitoring is so important, as it allows data scientists to detect any significant changes in model performance or accuracy.\\n\\n\"Management\" refers to the actions taken based on the results of monitoring. If monitoring reveals that the model\\'s performance is declining, it might need to be retrained with new data, refined, or replaced entirely. This is a critical part of maintaining the reliability and usefulness of machine learning applications in a real-world environment. \\n\\nIn addition, model monitoring can also help detect anomalies or biases, ensuring that models are fair and ethical. \\n\\nIn summary, \"Model Monitoring and Management\" helps to ensure that machine learning models continue to perform well and provide value after they have been deployed.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_with_summary.predict(input=\"can you give me 5th point with explaination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02f6acf6-9e1d-485b-83b5-99ef047416ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Hi, what\\'s up?\\nAI: Hello! I\\'m here, processing information and assisting users like you. How can I help you today?\\nHuman: Sunny and Mayank are working on a mlops production ready project.\\nAI: That\\'s interesting! MLOps, or DevOps for machine learning, is all about bridging the gap between the development (Dev) and operation (Ops) sides of machine learning. It\\'s a practice for collaboration and communication between data scientists and operations professionals to help manage production ML lifecycle. \\n\\nSunny and Mayank must be involved in tasks like building, testing, and deploying machine learning models efficiently and reliably. They may also be working on automating various steps in ML lifecycle such as data collection, model training, model serving, monitoring, etc. \\n\\nWould you like to know more about MLOps or details related to their project?\\nHuman: so can you describe mlops pipeline to me with in six point.\\nAI: Absolutely, here is a simplified description of an MLOps pipeline broken down into six points:\\n\\n1. **Data Collection and Preparation**: This is the first step where raw data is collected from various sources, cleaned, and transformed into a format that can be used by machine learning models.\\n\\n2. **Model Development and Training**: In this stage, data scientists use the prepared data to develop and train machine learning models. They experiment with different algorithms and tune parameters to find the most effective model.\\n\\n3. **Model Validation**: Once a model is trained, it is validated using a different set of data to assess its performance and accuracy. This can help identify any issues or biases in the model.\\n\\n4. **Model Deployment**: After validation, the model is deployed into a production environment. This might involve integrating the model into existing systems and setting up APIs for other applications to use.\\n\\n5. **Model Monitoring and Management**: After deployment, the model\\'s performance is continuously monitored to ensure it is still providing accurate predictions. If the model’s performance degrades, it may need to be re-trained with new data.\\n\\n6. **Retraining and Updating Models**: The models might need to be retrained and updated over time as new data becomes available or if the performance of the model starts to decline. This should be an automated process to ensure the model is always performing at its best.\\n\\nIt\\'s worth noting that MLOps aims to automate as many of these steps as possible to increase efficiency and reduce errors.\\nHuman: How many total points are there?\\nAI: In the description I provided, there are six main points or stages in the MLOps pipeline. However, depending on the complexity of the project and the specific needs of a team, there might be more detailed steps or sub-steps within each of these main stages. In general, the six stages I mentioned cover the essential elements of an MLOps pipeline.\\nHuman: can you give me 5th point with explaination\\nAI: Sure, the 5th point in the MLOps pipeline is \"Model Monitoring and Management\". \\n\\nAfter a machine learning model is deployed into a production environment, it doesn\\'t mean the work is done. It\\'s crucial to continuously monitor the model\\'s performance to ensure it is still providing accurate and reliable predictions. This is what we mean by \"Model Monitoring\".\\n\\nThe performance of a model might degrade over time due to various factors, including changes in the underlying data it was trained on, also known as concept drift. This is why ongoing monitoring is so important, as it allows data scientists to detect any significant changes in model performance or accuracy.\\n\\n\"Management\" refers to the actions taken based on the results of monitoring. If monitoring reveals that the model\\'s performance is declining, it might need to be retrained with new data, refined, or replaced entirely. This is a critical part of maintaining the reliability and usefulness of machine learning applications in a real-world environment. \\n\\nIn addition, model monitoring can also help detect anomalies or biases, ensuring that models are fair and ethical. \\n\\nIn summary, \"Model Monitoring and Management\" helps to ensure that machine learning models continue to perform well and provide value after they have been deployed.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversation_with_summary.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4532d-a633-4c25-b2b5-ed3954a952d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
