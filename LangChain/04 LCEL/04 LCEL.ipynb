{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5be90ad-70ad-4c0c-9543-ddc5b31f3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    " # It is syntax within the LangChain framework to build and orchestrate chains of operations involving language models, tools, and other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71d18d1d-71f5-4286-ab64-c6b931dd1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "    # - Superfast development of chains.\n",
    "    # - Advanced features such as streaming, async, parallel execution, and more.\n",
    "    # - Easy integration with LangSmith and LangServe.\n",
    "    # - Optimization\n",
    "        # - Parallel Execution: Run multiple components or inputs concurrently.\n",
    "        # - Async Support: Chains can be executed asynchronously.\n",
    "        # Streaming: Supports streaming outputs for faster response times.\n",
    "\n",
    "# Ex: \n",
    "\n",
    "    # chain = prompt | model | output_parser\n",
    "    # result = chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3e88c-d431-4d26-bd23-1142b6b4a1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e660e784-cdc6-4ae5-85e1-ccb49d6789d0",
   "metadata": {},
   "source": [
    "| Type              | Description                                      | Use Case Example                          |\n",
    "|-------------------|--------------------------------------------------|-------------------------------------------|\n",
    "| Linear            | Simple left-to-right chaining                    | Prompt → Model → Parser                   |\n",
    "| Sequential        | Multi-step with intermediate logic               | Multi-turn conversations                  |\n",
    "| Parallel          | Run multiple chains at once                      | Summarize + Extract keywords              |\n",
    "| Branching         | Conditional logic to choose chain                | Route to math or QA chain                 |\n",
    "| Map-Reduce        | Process many inputs and combine results          | Document summarization                    |\n",
    "| Streaming         | Output tokens as they are generated              | Chatbots, real-time apps                  |\n",
    "| Tool-Using        | Use external tools in the chain                  | Search, calculator, API calls             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022a663-8ad9-4617-9b44-257baed0d9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103afc7-34d7-4cdc-85c7-157296d5124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 01 Linear Type of LCEL\n",
    "\n",
    "# A Linear LCEL chain is a series of steps where:\n",
    "\n",
    "    # Each component takes the output of the previous one as its input.\n",
    "    # The flow is strictly left-to-right.\n",
    "    # There’s no branching, looping, or parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b273a9f-2e26-44fd-8ed8-932d6a44d60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b424b40-8afd-4cbf-8e12-37db410172de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Transitioning from Old class to New Pipe Base Operator\n",
    "\n",
    "# ## 1. Understanding `Runnables`\n",
    "# - `Runnables` are self-contained units of work.\n",
    "# - Can be executed in isolation or combined for complex operations.\n",
    "# - Provides flexibility in execution (sync, async, parallel).\n",
    "\n",
    "# ## 2. `RunnableParallel`\n",
    "# - Executes tasks concurrently.\n",
    "# - Useful for performance enhancement in scenarios where tasks can run independently.\n",
    "# - Syntax example:\n",
    "#     ```python\n",
    "#     from some_module import RunnableParallel\n",
    "#     ```\n",
    "\n",
    "# ## 3. `RunnablePassthrough`\n",
    "# - A simple `Runnable` that passes inputs directly to outputs without modification.\n",
    "# - Helpful for debugging or chaining in pipelines.\n",
    "# - Example use case:\n",
    "#     ```python\n",
    "#     from some_module import RunnablePassthrough\n",
    "#     passthrough = RunnablePassthrough()\n",
    "#     result = passthrough.run(input_data)\n",
    "#     ```\n",
    "\n",
    "# ## 4. `RunnableLambda`\n",
    "# - Allows quick, inline definitions of small, custom functions.\n",
    "# - Example:\n",
    "#     ```python\n",
    "#     from some_module import RunnableLambda\n",
    "#     lambda_op = RunnableLambda(lambda x: x * 2)\n",
    "#     result = lambda_op.run(5)  # Output: 10\n",
    "#     ```\n",
    "\n",
    "# ## 5. Assign Functions\n",
    "# - Used to assign values or parameters during execution.\n",
    "# - Useful in data pipelines to update intermediate values.\n",
    "\n",
    "# ## 6. Performance Improvement (Inference Speed)\n",
    "# - Focus on optimizing the inference speed by leveraging parallel execution.\n",
    "# - Use `RunnableParallel` or batching techniques.\n",
    "# - Consider optimizing data pipelines by removing unnecessary steps.\n",
    "\n",
    "# ## 7. Async Invoke\n",
    "# - Executes operations asynchronously, improving the overall throughput of the system.\n",
    "# - Syntax example:\n",
    "#     ```python\n",
    "#     async def async_operation():\n",
    "#         result = await some_async_function()\n",
    "#     ```\n",
    "\n",
    "# ## 8. Batch Support\n",
    "# - Handles multiple inputs at once to improve performance.\n",
    "# - Can be combined with `RunnableParallel` for parallel batch execution.\n",
    "\n",
    "# ## 9. Async Batch Execution\n",
    "# - Combines asynchronous execution with batch processing for high-performance tasks.\n",
    "# - Reduces overall execution time for larger datasets.\n",
    "\n",
    "# ## 10. Using `Itemgetter` with `LCEL`\n",
    "# - `Itemgetter` is used to extract specific items from collections.\n",
    "# - When combined with `LCEL` (LangChain Execution Layer), it can streamline complex operations.\n",
    "\n",
    "# ## 11. Bind Tools\n",
    "# - `Bind` tools help to connect different steps in the pipeline.\n",
    "# - Ensures smooth data flow between various `Runnable` components.\n",
    "\n",
    "# ## 12. Stream Support\n",
    "# - Keep your pipelines more responsive by incorporating stream support for data.\n",
    "# - This allows continuous data processing and near real-time outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a15bb9-ab22-423d-af1c-65ca181c5648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac798c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "from utils import (get_model_openAI,get_model_groq,get_openAI_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2fd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = read_json(\"keys.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06edc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the OpenAI chat model\n",
    "llm = get_model_openAI(model =\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e7a0b8-829b-4a1d-9737-8e333bc42d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't cats play poker in the jungle?\n",
      "\n",
      "Because there's too many cheetahs!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "model = get_model_openAI(model =\"gpt-4\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "result = chain.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92bf33fb-8511-4edc-8c75-ca514d7fd39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why don\\'t some adjectives ever go to dinner?\\n\\nBecause they\\'re too \"superficial\".'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"], \n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"your adjective here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e102c75-aa25-4375-9b96-52fd0390d627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77c3f507-890a-414e-b1eb-6283bd24e7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down?\\n\\nBecause it had too many sundaes!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463169e5-1547-4cf4-b835-9db0d4dec529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a722f5fa-2b92-46ac-a536-83861808630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering based on: ['doc1', 'doc2'] for question: Can you elaborate on: Python??\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use Case: Smart Q&A with Conditional Rephrasing  where 'intermediate logic' is needed\n",
    "# Let’s say you’re building a Q&A system where:\n",
    "\n",
    "    # 1.If the user’s question is too short or vague, you rephrase it to make it clearer.\n",
    "    # 2.Then you retrieve documents based on the (possibly rephrased) question.\n",
    "    # 3.Finally, you generate an answer using an LLM.\n",
    "\n",
    "# This requires intermediate logic to:\n",
    "    \n",
    "    # Check the length or clarity of the question.\n",
    "    # Decide whether to rephrase it or not.\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "\n",
    "# Step 1: Intermediate logic to rephrase if needed\n",
    "def maybe_rephrase(input):\n",
    "    question = input[\"question\"]\n",
    "    if len(question.split()) < 5:\n",
    "        # Rephrase short questions\n",
    "        return {\"question\": f\"Can you elaborate on: {question}?\"}\n",
    "    return {\"question\": question}\n",
    "\n",
    "rephrase_logic = RunnableLambda(maybe_rephrase)\n",
    "\n",
    "# Step 2: Document retrieval (mocked here)\n",
    "retriever = RunnableLambda(lambda x: {\"docs\": [\"doc1\", \"doc2\"], \"question\": x[\"question\"]})\n",
    "\n",
    "# Step 3: Answer generation (mocked)\n",
    "answer_generator = RunnableLambda(lambda x: f\"Answering based on: {x['docs']} for question: {x['question']}\")\n",
    "\n",
    "# Full chain\n",
    "chain = RunnableSequence(rephrase_logic,retriever,answer_generator)\n",
    "\n",
    "# Run it\n",
    "result = chain.invoke({\"question\": \"Python?\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69babd79-0b13-4e0f-b5e2-21a3f4e26fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772563dd-c2ba-411a-91ba-92c78442ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 : Sequential LCEL\n",
    "    # Use Sequential LCEL when your task involves multiple stages, intermediate logic, or complex data flow.\n",
    "    # Allows intermediate logic and transformations in between steps\n",
    "\n",
    "\n",
    "# When to Use Sequential LCEL\n",
    "    # When you need intermediate processing between steps\n",
    "    # When steps have multiple inputs or outputs\n",
    "    # When you want clear structure and traceability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94220897-564f-439d-8729-8eb7ab75f914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# In LangChain, a RunnableSequence is a powerful composition operator that allows you to chain together multiple runnables—components like \n",
    "# prompt templates, models, and output parsers—so that the output of one becomes the input of the next.\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def mul_two(x: int) -> int:\n",
    "    return x * 2\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(mul_two)\n",
    "\n",
    "# Create a sequence\n",
    "sequence = runnable_1 | runnable_2\n",
    "\n",
    "# Run it\n",
    "result = sequence.invoke(1)  # Output: 4\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f40c81-9130-45d6-a4ad-a27bcf60e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why did the lawyer go to the bank? To get his case settled!\n",
      "\n",
      "2. How many lawyer jokes are there? Only three. The rest are true stories.\n",
      "\n",
      "3. Why did the lawyer wear a suit to the job interview? Because he wanted to make a good case for himself!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
    "\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "model  = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "# chain = prompt_template | model\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "# Create individual runnables (steps in the chain)\n",
    "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
    "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
    "parse_output = RunnableLambda(lambda x: x.content)\n",
    "\n",
    "# Create the RunnableSequence (equivalent to the LCEL chain)\n",
    "chain = RunnableSequence(first=format_prompt, middle=[invoke_model], last=parse_output)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "# Output\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9660-defa-4b3c-b6e8-c675ce05cd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae51c1c-dae0-455e-982f-c5d823ffcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In LangChain, a Parallel Chain (also known as RunnableParallel) is a type of chain that allows you to run multiple Runnables simultaneously, \n",
    "# rather than sequentially. \n",
    "\n",
    "# This is useful when you want to perform independent tasks at the same time and then combine their results.\n",
    "\n",
    "# What is a Parallel Chain?\n",
    "    # It executes multiple Runnables in parallel.\n",
    "    # Each Runnable receives the same input.\n",
    "    # The outputs are returned as a dictionary, mapping each Runnable to its result.\n",
    "\n",
    "# Why Use It?\n",
    "    # To speed up workflows by running tasks concurrently.\n",
    "    # To gather multiple perspectives or outputs from different models/tools.\n",
    "    # To fan out a single input to multiple processing paths.\n",
    "\n",
    "# Real-World Use Case\n",
    "    # Imagine you want to:\n",
    "        # Summarize a document.\n",
    "        # Extract keywords.\n",
    "        # Translate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d78dd7-a7ad-4508-b052-cfa9f6bfeb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upper': 'Uppercase: LANGCHAIN', 'reverse': 'Reversed: niahcgnal'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "# Define two simple Runnables\n",
    "r1 = RunnableLambda(lambda x: f\"Uppercase: {x.upper()}\")\n",
    "r2 = RunnableLambda(lambda x: f\"Reversed: {x[::-1]}\")\n",
    "\n",
    "# Create a parallel chain\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"upper\": r1,\n",
    "    \"reverse\": r2\n",
    "})\n",
    "\n",
    "# Run it\n",
    "result = parallel_chain.invoke(\"langchain\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb86f779-6640-4c32-b51a-0db22264ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import SimpleJsonOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"In JSON format, give me a list of {topic} and their names in French, Spanish, and Cat Language.\"\n",
    ")\n",
    "\n",
    "parser = SimpleJsonOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Streaming output\n",
    "async for chunk in chain.astream({'topic': 'colors'}):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e77b1c0b-0b37-4460-b3f1-cb48c2d5a0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pros:\n",
      "Based on the features provided, here are the pros of the MacBook Pro:\n",
      "\n",
      "1. **Retina display**: Provides a high-quality viewing experience with vibrant colors and sharp detail, making it ideal for visual tasks like graphic design or watching movies.\n",
      "   \n",
      "2. **Powerful performance**: Ensures fast and smooth operation, allowing for multitasking and handling demanding applications with ease.\n",
      "\n",
      "3. **Touch Bar**: Offers a convenient way to access shortcuts and controls, enhancing productivity and customization options for different tasks.\n",
      "\n",
      "4. **Thunderbolt 3 ports**: Enables high-speed data transfer and connectivity with a wide range of peripherals, providing flexibility for various usage scenarios.\n",
      "\n",
      "5. **macOS operating system**: Known for its user-friendly interface and seamless integration with other Apple devices, along with a wide selection of built-in apps for productivity and creativity.\n",
      "\n",
      "6. **Long battery life**: Provides up to 10 hours of usage on a single charge, allowing for extended productivity without needing frequent recharging.\n",
      "\n",
      "7. **Sleek and lightweight design**: Offers portability and ease of transport, making it convenient for users who are frequently on the move.\n",
      "\n",
      "8. **FaceTime HD camera**: Delivers high-quality video calls and conferences, ensuring clear and crisp visuals for communication purposes.\n",
      "\n",
      "9. **Touch ID**: Enhances security by enabling secure and convenient login options, as well as facilitating online purchases with ease.\n",
      "\n",
      "10. **Apple ecosystem integration**: Offers seamless connectivity with other Apple devices and services, allowing for effortless sharing and synchronization across multiple platforms.\n",
      "\n",
      "Cons:\n",
      "1. Retina display: Some users may find the high-resolution screen to be too reflective in certain lighting conditions, causing glare and reflections.\n",
      "2. Powerful performance: The Intel Core processors may generate considerable heat under heavy workloads, leading to potential overheating issues.\n",
      "3. Touch Bar: The Touch Bar feature may not be utilized by all users, making it more of a novelty than a practical addition for some.\n",
      "4. Thunderbolt 3 ports: The lack of traditional USB ports may require users to purchase additional adapters for connecting older devices.\n",
      "5. macOS operating system: Users who are accustomed to Windows or other operating systems may experience a learning curve when transitioning to macOS.\n",
      "6. Long battery life: Actual battery life may vary based on usage, and heavy tasks such as video editing or gaming can significantly reduce battery longevity.\n",
      "7. Sleek and lightweight design: The thin and lightweight design may sacrifice durability, potentially making the MacBook Pro more susceptible to physical damage.\n",
      "8. FaceTime HD camera: While the camera quality is good, some users may prefer a higher resolution or additional features such as privacy shutters.\n",
      "9. Touch ID: Some users may find the fingerprint sensor to be less secure compared to more advanced biometric authentication methods.\n",
      "10. Apple ecosystem integration: Users heavily invested in non-Apple devices may find limited compatibility and integration with the MacBook Pro, leading to a fragmented user experience.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define pros analysis step\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the pros of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "\n",
    "# Define cons analysis step\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the cons of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "\n",
    "# Combine pros and cons into a final review\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
    "\n",
    "\n",
    "# Simplify branches with LCEL\n",
    "pros_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "cons_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
    "\n",
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80df9d9-c714-4982-9fe3-e5c8a080640b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d174e66d-882b-4c05-80d6-3e75e575ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Type — Branching:\n",
    "# In LCEL (LangChain Expression Language), Branching refers to a pattern where the execution path splits based on \\\n",
    "# some condition or logic, allowing different chains or components to be run depending on the input.\n",
    "\n",
    "# What is Branching in LCEL?\n",
    "    # Branching allows you to:\n",
    "        # Route inputs to different chains based on conditions\n",
    "        # Handle different types of tasks (e.g., math vs. text)\n",
    "        # Customize behavior dynamically\n",
    "\n",
    "# Prompt Templates for Different Feedback Types:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7a6bc1-b651-42c7-8c04-ea066f1a8620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ It's always a good time to learn!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "\n",
    "# Define condition functions\n",
    "def is_math(input):\n",
    "    return \"calculate\" in input[\"question\"].lower()\n",
    "\n",
    "def is_greeting(input):\n",
    "    return input[\"question\"].lower() in [\"hi\", \"hello\", \"hey\"]\n",
    "\n",
    "def is_time_question(input):\n",
    "    return \"time\" in input[\"question\"].lower()\n",
    "\n",
    "# Define mock chains\n",
    "math_chain = RunnableLambda(lambda x: \"🧮 Using calculator...\")\n",
    "greeting_chain = RunnableLambda(lambda x: \"👋 Hello! How can I help you?\")\n",
    "time_chain = RunnableLambda(lambda x: \"⏰ It's always a good time to learn!\")\n",
    "qa_chain = RunnableLambda(lambda x: \"💬 Using language model for general Q&A...\")\n",
    "\n",
    "# Branching logic\n",
    "branch = RunnableBranch(\n",
    "    (is_math, math_chain),\n",
    "    (is_greeting, greeting_chain),\n",
    "    (is_time_question, time_chain),\n",
    "    qa_chain  # default\n",
    ")\n",
    "\n",
    "# Example run\n",
    "result = branch.invoke({\"question\": \"What time is it?\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420136b-a98c-4fb9-b98d-431132e8ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define prompt templates for different feedback types\n",
    "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the feedback classification template\n",
    "classification_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the runnable branches for handling feedback\n",
    "branches = RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x,\n",
    "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x,\n",
    "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x,\n",
    "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
    "    ),\n",
    "    escalate_feedback_template | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create the classification chain\n",
    "classification_chain = classification_template | model | StrOutputParser()\n",
    "\n",
    "# Combine classification and response generation into one chain\n",
    "chain = classification_chain | branches\n",
    "\n",
    "# Run the chain with an example review\n",
    "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
    "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
    "\n",
    "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "result = chain.invoke({\"feedback\": review})\n",
    "\n",
    "# Output the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8a026-52fc-4d56-9137-7d9909fdcbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b55f23-26db-43b7-87be-5a8445484166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Map-Reduce in LCEL?\n",
    "    # Map Phase: Apply a chain to each item in a list (e.g., each document).\n",
    "    # Reduce Phase: Combine the outputs into a single result (e.g., a summary or answer).\n",
    "\n",
    "# the Map-Reduce pattern is used to process multiple inputs in parallel and then combine the results into a final output. \n",
    "# This is especially useful for tasks like:\n",
    "    # Document summarization\n",
    "    # Multi-document question answering\n",
    "    # Aggregating results from multiple sources\n",
    "\n",
    "\n",
    "# Use Case: Multi-Document Summarization\n",
    "    # You have a list of documents, and you want to:\n",
    "        # Summarize each document individually (Map phase)\n",
    "        # Combine those summaries into a single final summary (Reduce phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20e95f-33fb-4a8c-a4a0-4d5a6fd2f3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd23e712-711d-4e81-acb7-7d8c40fa84fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: The sentiment of the sentence is positive. The use of words such as \"exceeded my expectations\" and \"great quality\" convey a sense of satisfaction and delight, indicating a positive emotional tone towards the product.\n",
      "Summary: The product surpassed expectations and is of excellent quality.\n",
      "Original: The product exceeded my expectations. Great quality!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Create two different analysis prompts\n",
    "sentiment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a sentiment analysis expert. Analyze the emotional tone.\"),\n",
    "    (\"user\", \"What's the sentiment of: {text}\")\n",
    "])\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a summarization expert.\"),\n",
    "    (\"user\", \"Summarize in one sentence: {text}\")\n",
    "])\n",
    "\n",
    "# Use RunnableParallel to run both analyses simultaneously\n",
    "analysis_chain = RunnableParallel(\n",
    "    {\n",
    "        \"sentiment\": sentiment_prompt | model | StrOutputParser(),\n",
    "        \"summary\": summary_prompt | model  | StrOutputParser(),\n",
    "        \"original\": RunnablePassthrough()  # Pass through the original input\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test it\n",
    "sample_text = {\"text\": \"The product exceeded my expectations. Great quality!\"}\n",
    "results = analysis_chain.invoke(sample_text)\n",
    "\n",
    "print(\"Sentiment:\", results[\"sentiment\"])\n",
    "print(\"Summary:\", results[\"summary\"])\n",
    "print(\"Original:\", results[\"original\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234f452-e132-4c6e-9acd-c94d718fd4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9db741a3-86a2-4a45-bf1b-a96ab63e9367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One potential business model idea for Rainbow Threads could be to offer a subscription-based service where customers receive a monthly curated box of colorful and unique threads for their crafting projects. Customers could choose from different subscription tiers based on their needs and budget, and each box could include a variety of high-quality threads in different colors and textures. This model would not only generate recurring revenue for Rainbow Threads but also provide customers with a convenient and exciting way to discover new threads for their projects. Additionally, Rainbow Threads could offer add-on products such as exclusive patterns or tutorials to further enhance the value of the subscription service.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "USER_INPUT = \"colorful socks\"\n",
    "\n",
    "prompt_template_product = \"What is a good name for a company that makes {product}?\"\n",
    "prompt_template_business = \"Give me the best business model idea for my company named: {company}\"\n",
    "\n",
    "chain = (\n",
    "  PromptTemplate.from_template(prompt_template_product)\n",
    "  | model\n",
    "  | {'company': RunnablePassthrough()}\n",
    "  | PromptTemplate.from_template(prompt_template_business)\n",
    "  | model\n",
    ")\n",
    "\n",
    "business_model_output = chain.invoke({'product': 'colorful socks'})\n",
    "\n",
    "business_model_output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7b2ac-e8a7-43e6-89f9-b396fcd71704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the code you shared, RunnablePassthrough() is a utility from LangChain that passes its input through unchanged.\n",
    "# It’s used when you want to retain or forward a piece of data in a chain without modifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19469d-8210-400a-b3d8-45beea2636b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d57e0af7-9c2a-4be9-a922-8d90c8153f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x * 2\n",
    "\n",
    "# wrap the functions with RunnableLambda\n",
    "add_five = RunnableLambda(add_five)\n",
    "multiply_by_two = RunnableLambda(multiply_by_two)\n",
    "\n",
    "chain = add_five | multiply_by_two\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3789b06-fcff-4895-abdf-fed9713b0fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Runnable:\n",
    "  def __init__(self, func):\n",
    "    self.func = func\n",
    "\n",
    "  def __or__(self, other):\n",
    "    def chained_func(*args, **kwargs):\n",
    "      # self.func is on the left, other is on the right\n",
    "      return other(self.func(*args, **kwargs))\n",
    "    return Runnable(chained_func)\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.func(*args, **kwargs)\n",
    "\n",
    "def add_ten(x):\n",
    "  return x + 10\n",
    "\n",
    "def divide_by_two(x):\n",
    "  return x / 2\n",
    "\n",
    "runnable_add_ten = Runnable(add_ten)\n",
    "runnable_divide_by_two = Runnable(divide_by_two)\n",
    "chain = runnable_add_ten | runnable_divide_by_two\n",
    "result = chain(8) # (8+10) / 2 = 9.0 should be the answer\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "884271d5-d990-47ca-976a-461724485c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the AI break up with his girlfriend? Because he couldn't handle her emotional baggage!\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "get_fact = RunnableLambda(extract_fact)\n",
    "\n",
    "chain = prompt | model | output_parser | get_fact\n",
    "\n",
    "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b52bde4-5961-4c34-b988-d5e405a5ef61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
