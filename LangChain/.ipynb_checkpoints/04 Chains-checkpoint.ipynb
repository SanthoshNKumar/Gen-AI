{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d662d218-e8cb-4134-bb86-b326fa30d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "from utils import (get_model_openAI,get_model_groq,get_openAI_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6e5a9-3545-4290-938e-f507faa15887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd43122-bada-4133-9142-0355569651cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = read_json(\"keys.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af1f70a-9595-4dfc-a964-e041b0aca0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain\n",
    "###  a Chain is a sequence of steps that connects language models (LLMs) with other components like \n",
    "###   prompts, tools, memory, or APIs to perform more complex tasks than a single LLM call can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30651af6-93ff-4002-9b61-8d09caec5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of chains\n",
    "    # 1. LLMChain – Basic Prompt + LLM\n",
    "    # 2. SimpleSequentialChain – One Output Feeds into Next Input\n",
    "    # 3. SequentialChain – More Complex, Handles Multiple Inputs/Outputs\n",
    "    # 4. RouterChain / MultiPromptChain – Route Input to the Right Chain\n",
    "    # 5. MultiPrompt Chain - Uses different prompts for different tasks or personas.\n",
    "    # 6. RetrievalQA Chain - ombines a retriever (like a vector store) with an LLM to answer questions based on documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285026fb-eecf-4e6d-ad18-ddf9e6b135d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Use Chains?\n",
    "    ## Chains help you:\n",
    "        # Modularize complex tasks.\n",
    "        # Reuse components.\n",
    "        # Add memory, tools, or APIs.\n",
    "        # Control flow (e.g., conditionals, loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bf2ea-db35-4908-8485-782988baf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of Chain Composition\n",
    "    # 1. Sequential Chains (| operator)\n",
    "    # 2. Parallel\n",
    "    # 3. Complex Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ce2be-1a17-4d5e-8482-c6fa9fbd65e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a14c8-8b62-419e-bff2-70819cd22c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55eef7-54ce-47dd-b145-4b79a1f366d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426f060-ac09-4e94-a8b6-77b094f4421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. LLMChain – Basic Prompt + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a869b311-9f8c-459c-b21f-ee9d6d14be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. Unlike classical computers, which use bits to represent information as either 0 or 1, quantum computers use quantum bits, or qubits, which can represent both 0 and 1 at the same time.\n",
      "\n",
      "This allows quantum computers to perform multiple calculations simultaneously, making them potentially much faster and more powerful than classical computers for certain tasks. Quantum computing has the potential to revolutionize fields such as cryptography, drug discovery, and artificial intelligence by solving complex problems that are currently beyond the capabilities of classical computers.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "llm = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "# 2. Create a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the concept of {topic} in simple terms.\"\n",
    ")\n",
    "\n",
    "# 3. Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 4. Run the chain\n",
    "response = chain.run(\"quantum computing\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# What’s Happening Here?\n",
    "    # PromptTemplate: Defines how the input is formatted before being sent to the LLM.\n",
    "    # LLMChain: Combines the prompt and the LLM into a reusable chain.\n",
    "    # chain.run(): Executes the chain with a specific input (\"quantum computing\" in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34ef78-cb8b-4e6d-bdba-a1b80db3cf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "167d7c94-f2b3-41bf-91cc-7386bc43cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Step 1: Define the prompt template\n",
    "template = \"\"\"\n",
    "You are a helpful assistant that provides concise and informative answers.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "llm = get_model_openAI(model =\"gpt-4\")\n",
    "\n",
    "# Step 3: Create the LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Step 4: Run the chain with a specific input\n",
    "response = chain.run(question=\"What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ba19e-5f9c-4304-9e77-23dd2509fdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd7ac3e-5e5c-4ff3-a2bc-c0e787952730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ಶುಭೋದಯ\" (Shubhodaya)\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate\n",
    "\n",
    "#==== Using OpenAI Chat API =======\n",
    "# Initialize the OpenAI chat model\n",
    "open_ai = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "# LLMChain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"language\"],\n",
    "    template=\"How do you say good morning in {language}\"\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=open_ai, prompt=prompt)\n",
    "print(chain.run(language=\"kannada\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87ab75-f4b7-48a9-b856-c4b40e9d9f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0c6001b-9818-4280-8a4c-c5d0c2aea909",
   "metadata": {},
   "source": [
    "#### 02  SimpleSequentialChain – One Output Feeds into Next Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41384370-7e83-493f-9665-96ca1a40b109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mEcoHydrate\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\"Quenching your thirst sustainably.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\"Quenching your thirst sustainably.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# First chain: generate a company name\n",
    "prompt1 = PromptTemplate.from_template(\"Suggest a name for a company that makes {product}.\")\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "\n",
    "# Second chain: write a tagline for the company\n",
    "prompt2 = PromptTemplate.from_template(\"Write a tagline for a company named {company_name}.\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "\n",
    "# Combine them\n",
    "sequential_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "response = sequential_chain.run(\"eco-friendly water bottles\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ee111-a878-443a-accd-1ae9d3aab5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f22caab-7850-4187-8b42-471878e74f00",
   "metadata": {},
   "source": [
    "#### 3. SequentialChain – More Complex, Handles Multiple Inputs/Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4c30ac2-87e9-47ed-a09b-7d914af16b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m \n",
      " As a children's book writer, please come up with a simple and short (90 words)\n",
      " lullaby based on the location\n",
      " Zanzibar\n",
      " and the main character Maya\n",
      "\n",
      " STORY:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m \n",
      " As a children's book writer, please come up with a simple and short (90 words)\n",
      " lullaby based on the location\n",
      " Magical\n",
      " and the main character Karyna\n",
      "\n",
      " STORY:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "English Version ====> In a land of magic and whimsy, Karyna drifts off to sleep\n",
      "Underneath the starry sky, where dreams are free to keep\n",
      "With her wand and spellbook near, she casts away her fears\n",
      "And whispers to the moon above, wiping away her tears\n",
      "Close your eyes, sweet Karyna, let the magic take you far\n",
      "To a world of wonder and delight, where wishes are like stars\n",
      "Sleep tight, dear child, in this enchanted land so grand\n",
      "For in your dreams, you'll find the magic that you planned. \n",
      " \n",
      "\n",
      "Translated Version ====> В волшебной и веселой стране Карина засыпает\n",
      "Под звездным небом, где мечты свободно хранятся\n",
      "С волшебной палочкой и книгой заклинаний рядом, она отгоняет свои страхи\n",
      "И шепчет Луне наверху, вытирая слезы\n",
      "Закрой глаза, милая Карина, пусть магия унесет тебя далеко\n",
      "В мир удивления и восторга, где желания как звезды\n",
      "Спи крепко, дорогое дитя, в этой великолепной волшебной стране\n",
      "Ведь в твоих мечтах ты найдешь магию, которую запланировала.\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "open_ai = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "template = \"\"\" \n",
    " As a children's book writer, please come up with a simple and short (90 words)\n",
    " lullaby based on the location\n",
    " {location}\n",
    " and the main character {name}\n",
    " \n",
    " STORY:\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(input_variables=[\"location\", \"name\"],\n",
    "                        template=template)\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain_story = LLMChain(llm=open_ai, prompt=prompt, \n",
    "                       output_key=\"story\",\n",
    "                       verbose=True)\n",
    "\n",
    "# Chain 1\n",
    "story = chain_story({\"location\": \"Zanzibar\", \"name\": \"Maya\"})\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "open_ai = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_translate = PromptTemplate(input_variables=[\"story\", \"language\"],\n",
    "                                  template=template_update)\n",
    "\n",
    "# Chain 2\n",
    "from langchain.chains import LLMChain\n",
    "chain_translate = LLMChain(\n",
    "    llm=open_ai,\n",
    "    prompt=prompt_translate,\n",
    "    output_key=\"translated\"\n",
    ")\n",
    "\n",
    "# Sequential Chain\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "# ==== Create the Sequential Chain ===\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_story, chain_translate],\n",
    "    input_variables=[\"location\", \"name\", \"language\"],\n",
    "    output_variables=[\"story\", \"translated\"], # return story and the translated variables!\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = overall_chain({\"location\": \"Magical\", \n",
    "                          \"name\": \"Karyna\",\n",
    "                          \"language\": \"Russian\"\n",
    "                          })\n",
    "print(f\"English Version ====> { response['story']} \\n \\n\")\n",
    "print(f\"Translated Version ====> { response['translated']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dd5bb-5aaa-417d-954d-de31a74976ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5392e70-4dcf-40e7-b306-8e623db1965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load the document as a string\n",
    "context = '''A phenotype refers to the observable physical\n",
    "properties of an organism, including its appearance, development, and behavior.\n",
    "It is determined by both the organism's genotype, which is the set of genes\n",
    "it carries, and environmental influences upon these genes.'''\n",
    "\n",
    "# Create the Prompt Template for base qa_chain\n",
    "qa_template = \"\"\"Context information is below.\n",
    "    ---------------------\n",
    "    {context}\n",
    "    ---------------------\n",
    "    Given the context information and not prior knowledge, \n",
    "    answer the question: {question}\n",
    "    Answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=qa_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain = LLMChain(llm=get_model_openAI(model=\"gpt-4\"), prompt=PROMPT)\n",
    "\n",
    "query = \"What's a phenotype?\"\n",
    "\n",
    "chain({\"context\": context, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e22d91-f788-4035-b0e2-656b2f2422ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09aa6674-200f-4b7a-903d-0e8908267bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Paulo, I am an AI assistant and I do not have a personal name like a human. You can call me Assistant or anything else you prefer. How can I assist you today?\n",
      "I'm sorry, I do not have access to personal information about users.\n",
      "***********************************************************\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello there, I am Paulo\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello there, I am Paulo\n",
      "AI: Hello Paulo, nice to meet you! How can I assist you today?\n",
      "Human: Why is the sky blue?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello there, I am Paulo\n",
      "AI: Hello Paulo, nice to meet you! How can I assist you today?\n",
      "Human: Why is the sky blue?\n",
      "AI: The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it is made up of different colors, with blue light having shorter wavelengths that scatter more easily in the atmosphere. This scattering causes the blue light to be more visible to our eyes, giving the sky its blue color.\n",
      "Human: If phenomenon called Rayleigh didn't exist, what color would the sky be?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello there, I am Paulo\n",
      "AI: Hello Paulo, nice to meet you! How can I assist you today?\n",
      "Human: Why is the sky blue?\n",
      "AI: The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it is made up of different colors, with blue light having shorter wavelengths that scatter more easily in the atmosphere. This scattering causes the blue light to be more visible to our eyes, giving the sky its blue color.\n",
      "Human: If phenomenon called Rayleigh didn't exist, what color would the sky be?\n",
      "AI: Without Rayleigh scattering, the sky would actually appear violet. This is because violet light has even shorter wavelengths than blue light and would scatter even more easily in the atmosphere. So, in the absence of Rayleigh scattering, the sky would be violet in color instead of blue.\n",
      "Human: What's my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Memory ===> Human: Hello there, I am Paulo\n",
      "AI: Hello Paulo, nice to meet you! How can I assist you today?\n",
      "Human: Why is the sky blue?\n",
      "AI: The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it is made up of different colors, with blue light having shorter wavelengths that scatter more easily in the atmosphere. This scattering causes the blue light to be more visible to our eyes, giving the sky its blue color.\n",
      "Human: If phenomenon called Rayleigh didn't exist, what color would the sky be?\n",
      "AI: Without Rayleigh scattering, the sky would actually appear violet. This is because violet light has even shorter wavelengths than blue light and would scatter even more easily in the atmosphere. So, in the absence of Rayleigh scattering, the sky would be violet in color instead of blue.\n",
      "Human: What's my name?\n",
      "AI: Your name is Paulo. <====\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the OpenAI chat model\n",
    "llm = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "print(llm.predict(\"My name is Paulo. What is yours?\"))\n",
    "print(llm.predict(\"Great!  What's my name?\")) # we have memory issues!\n",
    "\n",
    "print(\"***********************************************************\")\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# How to solve llms memory issues?\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hello there, I am Paulo\")\n",
    "conversation.predict(input=\"Why is the sky blue?\")\n",
    "conversation.predict(input=\"If phenomenon called Rayleigh didn't exist, what color would the sky be?\")\n",
    "conversation.predict(input=\"What's my name?\")\n",
    "\n",
    "print(f\"Memory ===> {memory.buffer} <====\")\n",
    "\n",
    "# print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef146544-6a63-441d-8e4b-97d08083e7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5b0f45b-f7fe-48e7-87f0-9025b44c2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 75\n",
      "1. WHY DID THE LAWYER BRING A LADDER TO COURT? BECAUSE HE HEARD THE CASE WAS GOING TO BE HEARD ON A HIGHER LEVEL!\n",
      "\n",
      "2. HOW MANY LAWYERS DOES IT TAKE TO CHANGE A LIGHTBULB? THREE - ONE TO CLIMB THE LADDER, ONE TO SHAKE IT, AND ONE TO SUE THE LADDER COMPANY FOR FAULTY EQUIPMENT!\n",
      "\n",
      "3. WHY DO LAWYERS MAKE TERRIBLE ARCHAEOLOGISTS? BECAUSE THEY ALWAYS DIG UP THE PAST AND BILL YOU FOR IT!\n"
     ]
    }
   ],
   "source": [
    "# In LangChain, Chain Types define how different components (like prompts, models, and parsers) are connected and executed. The \"Extended\" \n",
    "# chain type isn't a formal term in the LangChain API, but it often refers to complex or custom chains that go beyond simple linear flows.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define prompt templates\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define additional processing steps using RunnableLambda\n",
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
    "\n",
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b1cdd-bc7e-4b69-8454-124a641dd29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c9ad4-4b1e-4fda-b778-f6743c442d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b821c64-3762-4fd8-a77e-126be2864a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c04f6465-cc7a-46a5-bbe8-eb5d7483e9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d4920-f96c-45d8-8d87-432ce0f1c6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf78ad2-a58c-487f-919e-f1e677ac7ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf9a82-2c50-44a8-8351-063604f25242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff7393b4-a87b-49b0-9655-e5edc79b6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Why did the lawyer bring a ladder to court? He heard the case was going to be heard on a higher level!\n",
      "\n",
      "2. How many lawyer jokes are there? Only three. The rest are all true stories.\n",
      "\n",
      "3. Why don't lawyers go to the beach? Cats keep trying to bury them in the sand!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811dad1-f91c-46ec-b5b6-f9c787aa35ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db0e5f30-1607-4394-95e1-7df392e2e4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01fd79-de2c-45be-9f6b-591cfbecfff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16bbfb25-468b-4fa7-ab50-2b37a9228037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upper': 'Uppercase: LANGCHAIN', 'reverse': 'Reversed: niahcgnal'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19756d-3cdf-4953-9201-2b8f65fedc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba496551-94d3-49f6-8a22-7e438f7c2dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d79de-af6d-4334-bfa3-98e973d6aee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9591e913-8372-4af5-821e-0f839d384a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pros:\n",
      "1. **Retina display**: The high-resolution display offers sharp and vibrant visuals, enhancing the overall viewing experience.\n",
      "2. **Powerful performance**: The Intel Core processors ensure smooth multitasking and efficient handling of demanding tasks.\n",
      "3. **Touch Bar**: The interactive touch bar provides quick access to frequently used tools and shortcuts, improving productivity.\n",
      "4. **Thunderbolt 3 ports**: High-speed data transfer and versatile connectivity options allow for seamless integration with various devices and peripherals.\n",
      "5. **macOS operating system**: Seamless integration with other Apple devices, a user-friendly interface, and a wide range of productivity and creative software applications.\n",
      "6. **Solid-state drive (SSD)**: Faster boot-up times, quicker file access, and improved overall system responsiveness compared to traditional hard drives.\n",
      "7. **FaceTime HD camera**: High-quality video calls and conferencing, ideal for remote work, online meetings, and staying connected with friends and family.\n",
      "8. **Touch ID**: Secure and convenient fingerprint authentication for quick and easy device access, as well as secure online transactions.\n",
      "9. **Long battery life**: Up to 10 hours of battery life ensures extended productivity without the need for frequent charging.\n",
      "10. **Sleek and lightweight design**: The thin and portable design makes it easy to carry the MacBook Pro anywhere, without compromising on performance or features.\n",
      "\n",
      "Cons:\n",
      "1. Retina display: May contribute to higher cost of the MacBook Pro compared to non-Retina display laptops.\n",
      "2. Powerful performance: Potential for overheating issues under heavy use.\n",
      "3. Touch Bar: Some users may find it less intuitive than traditional function keys.\n",
      "4. Thunderbolt 3 ports: Limited compatibility with older devices and accessories that may require adapters.\n",
      "5. macOS operating system: Limited software compatibility compared to Windows-based laptops.\n",
      "6. Solid-state drive (SSD): Higher cost compared to traditional hard drives for the same storage capacity.\n",
      "7. FaceTime HD camera: Quality may not be as superior as external webcams.\n",
      "8. Touch ID: Some users may prefer traditional password entry methods for security.\n",
      "9. Long battery life: Actual battery life may vary depending on usage and settings.\n",
      "10. Sleek and lightweight design: Potential for reduced durability compared to bulkier laptops.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ea4e1-9536-488e-a288-2c5af2119eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6227f9-ee28-4837-a9b5-eae8d1fdf3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97895b-a914-4ffa-9447-d0e52b31a5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6c919-cbde-42d2-b72c-667361c570c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5381de4-e07c-46b8-ad5f-e5967d5da8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d0a9c-b29e-44c6-abbc-f4129f2efb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
