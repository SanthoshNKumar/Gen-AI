{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f9af1ce-b952-4958-aba0-344bbd4224ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "from utils import (\n",
    "            get_model_openAI,\n",
    "            get_model_groq,\n",
    "            get_openAI_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5fa95-dd08-426b-8337-04035e1ca6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b79aee29-4ede-40d1-b812-48d960e471fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Types of LangChain Prompt Templates\n",
    "\n",
    "# - `PromptTemplate`  for creating basic prompts.\n",
    "# - `ChatPromptTemplate`  for chat-based prompts with multiple messages.\n",
    "# - `MessagesPlaceholder`  for injecting a dynamic list of messages (such as a conversation history).\n",
    "# - `FewShotPromptTemplate` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04bf0f-5920-4cef-a680-26f7a3408fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "987e5bb3-be81-4227-b415-999716e3c1ba",
   "metadata": {},
   "source": [
    "### ✅ Summary: `PromptTemplate` vs `ChatPromptTemplate`\n",
    "\n",
    "| Feature              | `PromptTemplate`                     | `ChatPromptTemplate`                          |\n",
    "|----------------------|--------------------------------------|------------------------------------------------|\n",
    "| **Model Type**       | Completion-based                     | Chat-based                                     |\n",
    "| **Input Format**     | Single string                        | List of messages with roles                   |\n",
    "| **Roles Supported**  | ❌ Not supported                     | ✅ Supported                                   |\n",
    "| **Use Case**         | Simple prompts                       | Multi-turn or role-specific prompts           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f0912-7aeb-4912-a31c-d021141fa387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7df24366-39f7-478f-ad56-949cbb909ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_model_openAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187b0b4-15a7-43e3-9365-0384a945b69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d74f972-75f7-4aa4-9018-5c16a483c3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"Hello\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3025f34-148f-459b-be9f-8de29a30e0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674925fa-7ce3-4b6f-8ece-b190ee36ed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])  \n",
    "response.content\n",
    "\n",
    "# Above format is not recommended instead use 'HumanMessage'; 'SystemMessage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df484b-2784-4eb3-ae2a-7752141bdc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23cb3ab-4ed9-4699-8907-11b34c93b3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Karnataka is Bangalore (officially known as Bengaluru).'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"What is the capital Karnataka\"\n",
    "llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661415c-2544-4165-b170-966637a7679d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4236dff-1018-4552-b284-e0b9fb42256a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Karnataka is Bangalore (officially known as Bengaluru).'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = \"Karnataka\"\n",
    "\n",
    "input = f\"What is the Capital of {state}\"\n",
    "llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7387353-38f9-48fd-bcca-3951634525cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c3ff68-f2e1-4ad3-9177-f16f69ab44c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karnataka=Bengaluru\\nTamil Nadu=Chennai'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state1 = \"Karnataka\"\n",
    "state2 = \"Tamil Nadhu\"\n",
    "\n",
    "input = f\"What is the Capital of {state1} and {state2}, write in the format of State=Capital with one below other\"\n",
    "llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e26619-6d91-474e-8fcc-6d78c95f7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bba8fe0-fdf8-4b73-bab6-9fd69b363bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Karnataka is Bangalore (officially known as Bengaluru).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat  = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Create a HumanMessage\n",
    "message = HumanMessage(content=\"What is the Capital of Karnataka.\")\n",
    "\n",
    "# Invoke the model with the message\n",
    "response = chat.invoke([message])\n",
    "\n",
    "# Print the response\n",
    "print(response.content)\n",
    "\n",
    "    # Why Use HumanMessage?\n",
    "    # It mimics a real chat interface.\n",
    "    # Useful for multi-turn conversations.\n",
    "    # You can also use SystemMessage and AIMessage for more control over the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad2628-b9b3-42cd-84fb-8f4042dc803e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f723a2f-631e-4568-a78a-753b22d8e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Quantum computing is a type of computation that uses quantum bits, or qubits, instead of the binary bits (0s and 1s) used in traditional computing. While a binary bit can be in one of two states (0 or 1), a qubit can be in both states at the same time, thanks to a principle called superposition. \n",
      "\n",
      "Another key principle of quantum physics that quantum computers use is entanglement, which allows qubits that are entangled to be linked together, such that the state of one can directly influence the other, no matter the distance between them.\n",
      "\n",
      "These features allow quantum computers to process a high number of possibilities simultaneously and solve certain problems much more efficiently than traditional computers. However, building quantum computers is quite challenging due to the fragile nature of qubits. But, once we overcome these challenges, quantum computing has the potential to revolutionize many fields, including cryptography, material science, and pharmaceuticals.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat  = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Define a system message to guide the assistant's behavior\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant that writes in a friendly and concise tone.\")\n",
    "\n",
    "# Define a human message (user input)\n",
    "human_message = HumanMessage(content=\"Can you explain what quantum computing is?\")\n",
    "\n",
    "# Send both messages to the model\n",
    "response = chat.invoke([system_message, human_message])\n",
    "\n",
    "# Print the model's response\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "    # What’s Happening:\n",
    "        # SystemMessage sets the persona or behavior of the assistant.\n",
    "        # HumanMessage is the actual user input.\n",
    "        # AIMessage is Represents a message from the AI (me)\n",
    "        # The model uses both to generate a context-aware response.\n",
    "        # They help structure the flow of a conversation between a user and an AI. Here's what each one means:\n",
    "\n",
    "    #  Why This Structure?\n",
    "        # Maintain conversation history.\n",
    "        # Control the tone and behavior of the AI.\n",
    "        # Build multi-turn conversations with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6316470-7d0e-4101-9461-842cbbd775ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff2e352-ffcd-4c28-a97a-73201941593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2021, the estimated population of Paris is about 2.16 million people within its city limits. However, the population in the wider Paris metropolitan area is estimated to be around 10.7 million.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat  = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Create a conversation history\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers questions clearly.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"Can you also tell me its population?\")\n",
    "]\n",
    "\n",
    "# Invoke the model with the full message history\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "# Print the model's response\n",
    "print(response.content)\n",
    "\n",
    "    ## What is Happening\n",
    "    # It(AIMessage) helps simulate multi-turn conversations.\n",
    "    # Useful when you want to maintain context or test how the model responds to a specific dialogue flow.\n",
    "    \n",
    "    # HumanMessage (for user input)\n",
    "    # SystemMessage (for system instructions)\n",
    "    # AIMessage (for previous AI responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada2395-5b64-4250-bc39-da47d8a44a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5feaa158-2a4e-43dd-867f-4e5ce721122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Key Features of Prompt Templates\n",
    "# 1. Dynamic Content:\n",
    "#     - Prompt Templates can include placeholders for variables that get filled in at runtime.\n",
    "#       Ex: For example, a template might look like \"Translate the following text to French: {text}\", where {text} is a \n",
    "#         variable that gets replaced with actual content.\n",
    "\n",
    "#     - Reusability\n",
    "#         - Prompt Templates are reusable components that can be applied to different tasks or datasets \n",
    "#           without needing to rewrite the prompt logic each time.\n",
    "\n",
    "#     - Consistent Formatting\n",
    "#         - Consistent prompts help in maintaining uniformity across different instances and use cases.\n",
    "\n",
    "#     - Customization and Flexibility\n",
    "#         - Templates can be customized for specific applications, such as question-answering, summarization, \n",
    "#           or creative writing, by altering the template structure and content.\n",
    "\n",
    "#     - Ease of Experimentation\n",
    "#         - With templates, experimenting with different prompt structures and phrasings becomes easier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e4c2e-1d8f-4d93-91bc-93329a2daece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47100fa3-621d-47a2-bbf0-13eb44bdb56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8faf3f18-7e99-4a1a-8bbc-fcd2a21e9946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karnataka=Bengaluru\n",
      "Tamil Nadu=Chennai\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "input = f\"What is the Capital of {state1} and {state2}, write in the format of State=Capital with one below other\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[state1,state2],\n",
    "    template = input,\n",
    ")\n",
    "\n",
    "# Format the prompt with actual values\n",
    "formatted_prompt = prompt.format(\n",
    "    state1=\"Karnataka\",\n",
    "    state2=\"Tamil Nadhu\"\n",
    ")\n",
    "\n",
    "llm = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Step 4: Directly invoke the LLM with the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Step 5: Print the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a763d-f058-461a-bbc6-86ad0b37fe04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81862be-bbe8-4554-a5c7-71fd06b23eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Creating a Chat Prompt Template with a Single Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d4dd37-2a39-4d43-8b7a-54a05242382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Prompt from Template-----\n",
      "messages=[HumanMessage(content='Tell me a joke about dogs.', additional_kwargs={}, response_metadata={})]\n",
      "Why did the scarecrow adopt a dog?\n",
      "\n",
      "Because he needed a \"barking\" buddy!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"Tell me a joke about {topic}.\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(\"-----Prompt from Template-----\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"dogs\"})\n",
    "print(prompt)\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c292ea56-4270-42b8-80cd-094a6d754847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Creating a Prompt with Multiple Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76e61524-bdea-45f9-930a-f9ab7ff25227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with Multiple Placeholders -----\n",
      "\n",
      "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "template_multiple = \"\"\"You are a helpful assistant.\n",
    "Human: Tell me a {adjective} story about a {animal}.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
    "\n",
    "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
    "\n",
    "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5406429b-f0b7-4038-95bd-64c2933a90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Creating Prompts with System and Human Messages Using Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33e52207-a21f-4a9f-ba92-0afe06a480b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1fe967-1c56-40e8-8dfa-850bec6be600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Creating Prompts with System and Human Messages Using Tuples with a static human message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6e52dc-bda5-423c-adb1-699e75ae30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    HumanMessage(content=\"Tell me 3 jokes.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\"})\n",
    "\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291ef38-0097-4ac9-bf09-e050a58de915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a851c302-c86e-4abb-8ecd-09fa254d1414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Kannada, good morning is said as \"ಶುಭೋದಯ\" (Shubhodaya).\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI chat model\n",
    "open_ai = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "# LLMChain\n",
    "formatted_prompt = PromptTemplate(\n",
    "    input_variables=[\"language\"],\n",
    "    template=\"How do you say good morning in {language}\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(formatted_prompt.format(language = \"Kannada\"))\n",
    "\n",
    "# Step 5: Print the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd18193-cbfd-4f4f-bb82-559689a1f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a82ad1c-683c-4cfa-bc63-36ce761ac346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke.\n",
      "Tell me a funny joke.\n",
      "Tell me a funny joke about chickens.\n",
      "Why don't chickens like people?\n",
      "\n",
      "Because they beat eggs!\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# No Input Variable\n",
    "no_input_prompt = PromptTemplate(\n",
    "    input_variables=[], \n",
    "    template=\"Tell me a joke.\"\n",
    ")\n",
    "\n",
    "print(no_input_prompt.format())\n",
    "\n",
    "# One Input Variable\n",
    "one_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"],\n",
    "    template=\"Tell me a {adjective} joke.\"\n",
    ")\n",
    "\n",
    "print(one_input_prompt.format(adjective=\"funny\"))\n",
    "\n",
    "# Multiple Input Variables\n",
    "multiple_input_prompt = PromptTemplate(\n",
    " input_variables=[\"adjective\", \"content\"],\n",
    " template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "multiple_input_prompt = multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(multiple_input_prompt)\n",
    "\n",
    "response = llm.invoke(\n",
    "    input =multiple_input_prompt\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b819-33a9-428c-95c2-0a194dd9408e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61422d-1189-4ac9-9b2f-f66e9a49fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PromptTemplate\n",
    "#     - Use when: You need a simple, reusable prompt with placeholders for variables.\n",
    "# 2. FewShotPromptTemplate\n",
    "#     - Use when: You want to guide the model with examples of how to respond.\n",
    "# 3. ChatPromptTemplate\n",
    "#     - Use when: You’re working with chat-based models (like GPT-4 or Claude) and need multi-turn conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aa655-7039-4396-ac47-cc12c9771618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2a6abd-5cfc-4fe3-99dc-742a8147fa23",
   "metadata": {},
   "source": [
    "# FewShotPromptTemplate\n",
    "\n",
    "FewShotPromptTemplate in LangChain when you want to guide a language model's behavior by showing it a few examples of how to respond to similar inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a58218-2c04-4b00-9d6f-56b556c803b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62d56a96-40e3-42fa-847b-398b272c5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Step 1: Define example formatting\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Q: {question}\\nA: {answer}\"\n",
    ")\n",
    "\n",
    "# Step 2: Provide few-shot examples\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n",
    "]\n",
    "\n",
    "# Step 3: Create the FewShotPromptTemplate\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Answer the following questions:\",\n",
    "    suffix=\"Q: {input}\\nA:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Step 4: Initialize the LLM\n",
    "llm = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Step 5: Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# Step 6: Run the chain with a new input\n",
    "response = chain.run(input=\"What is the capital of Italy?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb204e9-543c-4175-abb0-96dc85707880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c1a865a-a001-459a-8e6a-12a6f40a633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Create an FAQ in Markdown based on the following questions and answers:\\n\"\n",
    "        \"Q1: What is your return policy?\\n\"\n",
    "        \"A1: We accept returns within 30 days with receipt.\\n\"\n",
    "        \"Q2: Do you ship internationally?\\n\"\n",
    "        \"A2: Yes, we ship to over 50 countries.\\n\"\n",
    "        \"Q3: How can I track my order?\\n\"\n",
    "        \"A3: Use the tracking link in your confirmation email.\"\n",
    "    ),\n",
    "    input_variables=[]  # No variables needed yet\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({})  # Empty dict since no input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daaf6685-fa57-4063-a37d-a9c9579de09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9c18aaf-909e-4ea3-b3c8-d9dac0059259",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"\n",
    "Create a structured Markdown FAQ with anchor links, headers, and formatting conventions for readability.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Example input:\n",
    "Q1: What is your return policy?\n",
    "A1: We accept returns within 30 days with the original receipt.\n",
    "Q2: Do you ship internationally?\n",
    "A2: Yes, we ship to over 50 countries worldwide.\n",
    "Q3: How can I track my order?\n",
    "A3: After your order is shipped, you'll receive a tracking link via email.\n",
    "\n",
    "Example Output:\n",
    "\n",
    "# Frequently Asked Questions\n",
    "\n",
    "## [1. What is your return policy?](#1-what-is-your-return-policy)\n",
    "\n",
    "We accept returns within **30 days** with the original **receipt**.\n",
    "\n",
    "## [2. Do you ship internationally?](#2-do-you-ship-internationally)\n",
    "\n",
    "Yes, we ship to over **50 countries** worldwide.\n",
    "\n",
    "## [3. How can I track my order?](#3-how-can-i-track-my-order)\n",
    "\n",
    "After your order is shipped, you'll receive a **tracking link** via email.\n",
    "\n",
    "---\n",
    "\n",
    "Now generate the FAQ section for this data:\n",
    "Q1: What payment methods do you accept?\n",
    "A1: We accept Visa, Mastercard, PayPal, and Apple Pay.\n",
    "Q2: Can I change my shipping address after ordering?\n",
    "A2: Only if your order hasn't shipped yet. Contact support ASAP.\n",
    "Q3: Do you offer gift wrapping?\n",
    "A3: Yes! You can select gift wrapping during checkout.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate with no input variables (static prompt)\n",
    "prompt = PromptTemplate(template=prompt_str, input_variables=[])\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({})  # Empty dict since no input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327e0b5-12b3-4ec5-843d-ac1b529ea058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
