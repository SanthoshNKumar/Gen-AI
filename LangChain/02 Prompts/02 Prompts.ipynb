{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9af1ce-b952-4958-aba0-344bbd4224ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "from utils import (\n",
    "            get_model_openAI,\n",
    "            get_model_groq,\n",
    "            get_openAI_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5fa95-dd08-426b-8337-04035e1ca6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79aee29-4ede-40d1-b812-48d960e471fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Types of LangChain Prompt Templates\n",
    "\n",
    "# - `PromptTemplate`  for creating basic prompts.\n",
    "# - `ChatPromptTemplate`  for chat-based prompts with multiple messages.\n",
    "# - `MessagesPlaceholder`  for injecting a dynamic list of messages (such as a conversation history).\n",
    "# - `FewShotPromptTemplate` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04bf0f-5920-4cef-a680-26f7a3408fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "987e5bb3-be81-4227-b415-999716e3c1ba",
   "metadata": {},
   "source": [
    "### ✅ Summary: `PromptTemplate` vs `ChatPromptTemplate`\n",
    "\n",
    "| Feature              | `PromptTemplate`                     | `ChatPromptTemplate`                          |\n",
    "|----------------------|--------------------------------------|------------------------------------------------|\n",
    "| **Model Type**       | Completion-based                     | Chat-based                                     |\n",
    "| **Input Format**     | Single string                        | List of messages with roles                   |\n",
    "| **Roles Supported**  | ❌ Not supported                     | ✅ Supported                                   |\n",
    "| **Use Case**         | Simple prompts                       | Multi-turn or role-specific prompts           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f0912-7aeb-4912-a31c-d021141fa387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfda7c-7309-43c0-aa7a-99ecf187d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### What is a Prompt in LLMs?\n",
    "\n",
    "# A  `prompt`  is:\n",
    "\n",
    "# -   The  `textual input`  you provide to an LLM.\n",
    "# -   It can include  `instructions`,  `questions`,  `examples`, or  `context`.\n",
    "# -   It  `guides`  the model’s behavior and output.\n",
    "\n",
    "# ### Prompt = More Than Just Input\n",
    "\n",
    "# While it is technically the input, a  `well-crafted prompt`  often includes:\n",
    "\n",
    "# 1.  `Context`: Background information or prior conversation.\n",
    "# 2.  `Instructions`: What you want the model to do.\n",
    "# 3.  `Constraints`: Format, tone, length, etc.\n",
    "# 4.  `Examples`  _(optional)_: To show the model what kind of output you expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb64b3-39c0-4b92-9ca5-c86d7f05e257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5967b6-3a49-47ad-ad3b-5220da9221f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Sources of Prompts Beyond User Input\n",
    "\n",
    "# ### 1.  `Tools`\n",
    "\n",
    "# -   Tools like  `retrievers`,  `calculators`,  `code interpreters`, or  `search engines`  can generate intermediate prompts.\n",
    "\n",
    "# ### 2.  `Memory`\n",
    "\n",
    "# -   In systems with  `long-term memory`  or  `session memory`, past interactions can be used to  `auto-generate prompts`.\n",
    "# -   This helps maintain  `context`,  `personalization`, or  `task continuity`.\n",
    "\n",
    "# ### 3.  `System or Agent Frameworks`\n",
    "\n",
    "# -   In agent-based systems (like LangChain, AutoGPT, or custom agents), prompts can be:\n",
    "#     -   `Dynamically generated`  by the agent\n",
    "#     -   `Chained`  from previous outputs\n",
    "#     -   `Conditionally structured`  based on logic or goals\n",
    "\n",
    "# ### 4.  `Embedded Prompts in Applications`\n",
    "\n",
    "# -   Apps using LLMs often embed prompts in their backend logic.\n",
    "# -   These may include  `templates`,  `conditional logic`, or  `user metadata`.\n",
    "# -   `Example`:  \n",
    "#     _\"Generate a friendly onboarding message for a new user named Priya who signed up for a fitness app.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862940b-99c7-4ce4-bb45-c0a509c8ea35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad802c6-2401-4afa-a445-157bc05cc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## How LLMs Understand Prompts\n",
    "\n",
    "# ### 1.  `Tokenization`\n",
    "\n",
    "# -   The prompt is first  `broken down into tokens`  (words, subwords, or characters).\n",
    "# -   Example:  \n",
    "#     Prompt:  _\"Translate 'hello' to French.\"_  \n",
    "#     Tokens:  `[\"Translate\", \"'\", \"hello\", \"'\", \"to\", \"French\", \".\"]`\n",
    "\n",
    "# ### 2.  `Contextual Embedding`\n",
    "\n",
    "# -   Each token is converted into a  `vector`  (a list of numbers) that captures its  `meaning in context`.\n",
    "# -   These vectors are passed through multiple layers of the model.\n",
    "\n",
    "# ### 3.  `Attention Mechanism`\n",
    "\n",
    "# -   The model uses  `self-attention`  to understand how each word relates to others.\n",
    "# -   This helps it focus on the most relevant parts of the prompt.\n",
    "\n",
    "# ### 4.  `Pattern Recognition`\n",
    "\n",
    "# -   LLMs are trained on massive datasets and learn  `patterns`  in language.\n",
    "# -   When you give a prompt, the model tries to  `predict the most likely next token`  based on those patterns.\n",
    "\n",
    "# ### 5.  `Decoding`\n",
    "\n",
    "# -   The model generates a response  `one token at a time`, using probabilities to choose the most likely next word.\n",
    "# -   This continues until it reaches a stopping point (like a period or a max token limit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0430c-392f-4709-a0f0-5c46588e9580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f625416-253b-4f52-bfb5-8e46a3404b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prompt Optimization Techniques\n",
    "\n",
    "# ### 1.  `Role Prompting`\n",
    "    \n",
    "    # -   `Description`: Assign a role to the model to guide its behavior or tone.\n",
    "    # -   `Example`:  _\"You are a legal advisor. Explain the implications of this contract clause.\"_\n",
    "\n",
    "# ### 2.  `Chain-of-Thought Prompting`\n",
    "\n",
    "    # -   `Description`: Encourage the model to reason step-by-step to improve accuracy and logic.\n",
    "    # -   `Example`:  _\"Let's think step by step to solve this math problem.\"_\n",
    "\n",
    "# ### 3.  `Few-shot Prompting`\n",
    "\n",
    "    # -   `Description`: Provide a few examples of input-output pairs to guide the model.\n",
    "    # -   `Example`:  \n",
    "    #     _Input: Translate 'Hello' to French → Output: Bonjour_  \n",
    "    #     _Input: Translate 'Goodbye' to French → Output: Au revoir_\n",
    "\n",
    "# ### 4.  `Zero-shot Prompting`\n",
    "\n",
    "    # -   `Description`: Ask the model to perform a task without any examples.\n",
    "    # -   `Example`:  _\"Summarize this article in one paragraph.\"_\n",
    "    \n",
    "# ### 5.  `Instruction Prompting`\n",
    "\n",
    "    # -   `Description`: Use clear, direct instructions to specify the task.\n",
    "    # -   `Example`:  _\"List three benefits of solar energy in bullet points.\"_\n",
    "\n",
    "# ### 6.  `Output Constraints`\n",
    "\n",
    "    # -   `Description`: Define the format, tone, or length of the output.\n",
    "    # -   `Example`:  _\"Respond in JSON format with keys: 'title', 'summary', and 'keywords'.\"_\n",
    "    \n",
    "# ### 7.  `Self-Consistency Prompting`\n",
    "    \n",
    "    # -   `Description`: Generate multiple responses and select the most consistent or accurate one.\n",
    "    # -   `Example`:  _\"Generate three different answers and choose the most logical one.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e3d48-4902-4077-948c-99041a5f4a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df24366-39f7-478f-ad56-949cbb909ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_model_openAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187b0b4-15a7-43e3-9365-0384a945b69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d74f972-75f7-4aa4-9018-5c16a483c3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"Hello\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3025f34-148f-459b-be9f-8de29a30e0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674925fa-7ce3-4b6f-8ece-b190ee36ed8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])  \n",
    "response.content\n",
    "\n",
    "# Above format is not recommended instead use 'HumanMessage'; 'SystemMessage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df484b-2784-4eb3-ae2a-7752141bdc19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23cb3ab-4ed9-4699-8907-11b34c93b3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Karnataka is Bangalore (officially known as Bengaluru).'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"What is the capital Karnataka\"\n",
    "llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661415c-2544-4165-b170-966637a7679d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4236dff-1018-4552-b284-e0b9fb42256a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Karnataka is Bangalore (officially known as Bengaluru).'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = \"Karnataka\"\n",
    "\n",
    "input = f\"What is the Capital of {state}\"\n",
    "llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7387353-38f9-48fd-bcca-3951634525cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c3ff68-f2e1-4ad3-9177-f16f69ab44c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Karnataka=Bengaluru\\nTamil Nadu=Chennai'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state1 = \"Karnataka\"\n",
    "state2 = \"Tamil Nadhu\"\n",
    "\n",
    "input = f\"What is the Capital of {state1} and {state2}, write in the format of State=Capital with one below other\"\n",
    "llm.invoke(input).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e26619-6d91-474e-8fcc-6d78c95f7575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bba8fe0-fdf8-4b73-bab6-9fd69b363bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Karnataka is Bangalore (officially known as Bengaluru).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat  = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Create a HumanMessage\n",
    "message = HumanMessage(content=\"What is the Capital of Karnataka.\")\n",
    "\n",
    "# Invoke the model with the message\n",
    "response = chat.invoke([message])\n",
    "\n",
    "# Print the response\n",
    "print(response.content)\n",
    "\n",
    "    # Why Use HumanMessage?\n",
    "    # It mimics a real chat interface.\n",
    "    # Useful for multi-turn conversations.\n",
    "    # You can also use SystemMessage and AIMessage for more control over the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad2628-b9b3-42cd-84fb-8f4042dc803e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f723a2f-631e-4568-a78a-753b22d8e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Quantum computing is a type of computation that uses quantum bits, or qubits, instead of the binary bits (0s and 1s) used in traditional computing. While a binary bit can be in one of two states (0 or 1), a qubit can be in both states at the same time, thanks to a principle called superposition. \n",
      "\n",
      "Another key principle of quantum physics that quantum computers use is entanglement, which allows qubits that are entangled to be linked together, such that the state of one can directly influence the other, no matter the distance between them.\n",
      "\n",
      "These features allow quantum computers to process a high number of possibilities simultaneously and solve certain problems much more efficiently than traditional computers. However, building quantum computers is quite challenging due to the fragile nature of qubits. But, once we overcome these challenges, quantum computing has the potential to revolutionize many fields, including cryptography, material science, and pharmaceuticals.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat  = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Define a system message to guide the assistant's behavior\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant that writes in a friendly and concise tone.\")\n",
    "\n",
    "# Define a human message (user input)\n",
    "human_message = HumanMessage(content=\"Can you explain what quantum computing is?\")\n",
    "\n",
    "# Send both messages to the model\n",
    "response = chat.invoke([system_message, human_message])\n",
    "\n",
    "# Print the model's response\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "    # What’s Happening:\n",
    "        # SystemMessage sets the persona or behavior of the assistant.\n",
    "        # HumanMessage is the actual user input.\n",
    "        # AIMessage is Represents a message from the AI (me)\n",
    "        # The model uses both to generate a context-aware response.\n",
    "        # They help structure the flow of a conversation between a user and an AI. Here's what each one means:\n",
    "\n",
    "    #  Why This Structure?\n",
    "        # Maintain conversation history.\n",
    "        # Control the tone and behavior of the AI.\n",
    "        # Build multi-turn conversations with context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6316470-7d0e-4101-9461-842cbbd775ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff2e352-ffcd-4c28-a97a-73201941593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of 2021, the estimated population of Paris is about 2.16 million people within its city limits. However, the population in the wider Paris metropolitan area is estimated to be around 10.7 million.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "chat  = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Create a conversation history\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers questions clearly.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"Can you also tell me its population?\")\n",
    "]\n",
    "\n",
    "# Invoke the model with the full message history\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "# Print the model's response\n",
    "print(response.content)\n",
    "\n",
    "    ## What is Happening\n",
    "    # It(AIMessage) helps simulate multi-turn conversations.\n",
    "    # Useful when you want to maintain context or test how the model responds to a specific dialogue flow.\n",
    "    \n",
    "    # HumanMessage (for user input)\n",
    "    # SystemMessage (for system instructions)\n",
    "    # AIMessage (for previous AI responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada2395-5b64-4250-bc39-da47d8a44a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5feaa158-2a4e-43dd-867f-4e5ce721122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Key Features of Prompt Templates\n",
    "# 1. Dynamic Content:\n",
    "#     - Prompt Templates can include placeholders for variables that get filled in at runtime.\n",
    "#       Ex: For example, a template might look like \"Translate the following text to French: {text}\", where {text} is a \n",
    "#         variable that gets replaced with actual content.\n",
    "\n",
    "#     - Reusability\n",
    "#         - Prompt Templates are reusable components that can be applied to different tasks or datasets \n",
    "#           without needing to rewrite the prompt logic each time.\n",
    "\n",
    "#     - Consistent Formatting\n",
    "#         - Consistent prompts help in maintaining uniformity across different instances and use cases.\n",
    "\n",
    "#     - Customization and Flexibility\n",
    "#         - Templates can be customized for specific applications, such as question-answering, summarization, \n",
    "#           or creative writing, by altering the template structure and content.\n",
    "\n",
    "#     - Ease of Experimentation\n",
    "#         - With templates, experimenting with different prompt structures and phrasings becomes easier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47100fa3-621d-47a2-bbf0-13eb44bdb56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8faf3f18-7e99-4a1a-8bbc-fcd2a21e9946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karnataka=Bengaluru\n",
      "Tamil Nadu=Chennai\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "input = f\"What is the Capital of {state1} and {state2}, write in the format of State=Capital with one below other\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[state1,state2],\n",
    "    template = input,\n",
    ")\n",
    "\n",
    "# Format the prompt with actual values\n",
    "formatted_prompt = prompt.format(\n",
    "    state1=\"Karnataka\",\n",
    "    state2=\"Tamil Nadhu\"\n",
    ")\n",
    "\n",
    "llm = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Step 4: Directly invoke the LLM with the formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Step 5: Print the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a763d-f058-461a-bbc6-86ad0b37fe04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81862be-bbe8-4554-a5c7-71fd06b23eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Creating a Chat Prompt Template with a Single Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d4dd37-2a39-4d43-8b7a-54a05242382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Prompt from Template-----\n",
      "messages=[HumanMessage(content='Tell me a joke about dogs.', additional_kwargs={}, response_metadata={})]\n",
      "Why did the scarecrow adopt a dog?\n",
      "\n",
      "Because he needed a \"barking\" buddy!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"Tell me a joke about {topic}.\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(\"-----Prompt from Template-----\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"dogs\"})\n",
    "print(prompt)\n",
    "\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c292ea56-4270-42b8-80cd-094a6d754847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Creating a Prompt with Multiple Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76e61524-bdea-45f9-930a-f9ab7ff25227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with Multiple Placeholders -----\n",
      "\n",
      "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "template_multiple = \"\"\"You are a helpful assistant.\n",
    "Human: Tell me a {adjective} story about a {animal}.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
    "\n",
    "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
    "\n",
    "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5406429b-f0b7-4038-95bd-64c2933a90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Creating Prompts with System and Human Messages Using Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33e52207-a21f-4a9f-ba92-0afe06a480b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1fe967-1c56-40e8-8dfa-850bec6be600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Creating Prompts with System and Human Messages Using Tuples with a static human message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6e52dc-bda5-423c-adb1-699e75ae30b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    HumanMessage(content=\"Tell me 3 jokes.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\"})\n",
    "\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291ef38-0097-4ac9-bf09-e050a58de915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a851c302-c86e-4abb-8ecd-09fa254d1414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Kannada, good morning is said as \"ಶುಭೋದಯ\" (Shubhodaya).\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI chat model\n",
    "open_ai = get_model_openAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "# LLMChain\n",
    "formatted_prompt = PromptTemplate(\n",
    "    input_variables=[\"language\"],\n",
    "    template=\"How do you say good morning in {language}\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(formatted_prompt.format(language = \"Kannada\"))\n",
    "\n",
    "# Step 5: Print the result\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd18193-cbfd-4f4f-bb82-559689a1f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a82ad1c-683c-4cfa-bc63-36ce761ac346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke.\n",
      "Tell me a funny joke.\n",
      "Tell me a funny joke about chickens.\n",
      "Why don't chickens like people?\n",
      "\n",
      "Because they beat eggs!\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# No Input Variable\n",
    "no_input_prompt = PromptTemplate(\n",
    "    input_variables=[], \n",
    "    template=\"Tell me a joke.\"\n",
    ")\n",
    "\n",
    "print(no_input_prompt.format())\n",
    "\n",
    "# One Input Variable\n",
    "one_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"],\n",
    "    template=\"Tell me a {adjective} joke.\"\n",
    ")\n",
    "\n",
    "print(one_input_prompt.format(adjective=\"funny\"))\n",
    "\n",
    "# Multiple Input Variables\n",
    "multiple_input_prompt = PromptTemplate(\n",
    " input_variables=[\"adjective\", \"content\"],\n",
    " template=\"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "multiple_input_prompt = multiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "print(multiple_input_prompt)\n",
    "\n",
    "response = llm.invoke(\n",
    "    input =multiple_input_prompt\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b819-33a9-428c-95c2-0a194dd9408e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61422d-1189-4ac9-9b2f-f66e9a49fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PromptTemplate\n",
    "#     - Use when: You need a simple, reusable prompt with placeholders for variables.\n",
    "# 2. FewShotPromptTemplate\n",
    "#     - Use when: You want to guide the model with examples of how to respond.\n",
    "# 3. ChatPromptTemplate\n",
    "#     - Use when: You’re working with chat-based models (like GPT-4 or Claude) and need multi-turn conversations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aa655-7039-4396-ac47-cc12c9771618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2a6abd-5cfc-4fe3-99dc-742a8147fa23",
   "metadata": {},
   "source": [
    "#### FewShotPromptTemplate\n",
    "\n",
    "FewShotPromptTemplate in LangChain when you want to guide a language model's behavior by showing it a few examples of how to respond to similar inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a58218-2c04-4b00-9d6f-56b556c803b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62d56a96-40e3-42fa-847b-398b272c5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Step 1: Define example formatting\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Q: {question}\\nA: {answer}\"\n",
    ")\n",
    "\n",
    "# Step 2: Provide few-shot examples\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n",
    "]\n",
    "\n",
    "# Step 3: Create the FewShotPromptTemplate\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Answer the following questions:\",\n",
    "    suffix=\"Q: {input}\\nA:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Step 4: Initialize the LLM\n",
    "llm = get_model_openAI(model=\"gpt-4\")\n",
    "\n",
    "# Step 5: Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "\n",
    "# Step 6: Run the chain with a new input\n",
    "response = chain.run(input=\"What is the capital of Italy?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb204e9-543c-4175-abb0-96dc85707880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c1a865a-a001-459a-8e6a-12a6f40a633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Create an FAQ in Markdown based on the following questions and answers:\\n\"\n",
    "        \"Q1: What is your return policy?\\n\"\n",
    "        \"A1: We accept returns within 30 days with receipt.\\n\"\n",
    "        \"Q2: Do you ship internationally?\\n\"\n",
    "        \"A2: Yes, we ship to over 50 countries.\\n\"\n",
    "        \"Q3: How can I track my order?\\n\"\n",
    "        \"A3: Use the tracking link in your confirmation email.\"\n",
    "    ),\n",
    "    input_variables=[]  # No variables needed yet\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({})  # Empty dict since no input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daaf6685-fa57-4063-a37d-a9c9579de09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9c18aaf-909e-4ea3-b3c8-d9dac0059259",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"\n",
    "Create a structured Markdown FAQ with anchor links, headers, and formatting conventions for readability.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Example input:\n",
    "Q1: What is your return policy?\n",
    "A1: We accept returns within 30 days with the original receipt.\n",
    "Q2: Do you ship internationally?\n",
    "A2: Yes, we ship to over 50 countries worldwide.\n",
    "Q3: How can I track my order?\n",
    "A3: After your order is shipped, you'll receive a tracking link via email.\n",
    "\n",
    "Example Output:\n",
    "\n",
    "# Frequently Asked Questions\n",
    "\n",
    "## [1. What is your return policy?](#1-what-is-your-return-policy)\n",
    "\n",
    "We accept returns within **30 days** with the original **receipt**.\n",
    "\n",
    "## [2. Do you ship internationally?](#2-do-you-ship-internationally)\n",
    "\n",
    "Yes, we ship to over **50 countries** worldwide.\n",
    "\n",
    "## [3. How can I track my order?](#3-how-can-i-track-my-order)\n",
    "\n",
    "After your order is shipped, you'll receive a **tracking link** via email.\n",
    "\n",
    "---\n",
    "\n",
    "Now generate the FAQ section for this data:\n",
    "Q1: What payment methods do you accept?\n",
    "A1: We accept Visa, Mastercard, PayPal, and Apple Pay.\n",
    "Q2: Can I change my shipping address after ordering?\n",
    "A2: Only if your order hasn't shipped yet. Contact support ASAP.\n",
    "Q3: Do you offer gift wrapping?\n",
    "A3: Yes! You can select gift wrapping during checkout.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate with no input variables (static prompt)\n",
    "prompt = PromptTemplate(template=prompt_str, input_variables=[])\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({})  # Empty dict since no input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d201510-e9ef-4e5d-b251-33aaa44690b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22614e9-93d2-4b5d-8f54-4511a9b0729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Generated Answer: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input:  my name is santhosh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Generated Answer: Nice to meet you, Santhosh! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input:  may i know where Mysore comes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Generated Answer: Mysore, officially known as Mysuru, is a city in the southern part of the state of Karnataka, India. It is located at the base of the Chamundi Hills about 146 km southwest of the state capital, Bangalore. Mysore is known for its heritage structures and palaces, including the Mysore Palace, and for the festivities that take place during the Dasara festival when the city receives a large number of tourists from around the world.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input:  HOw about Mirle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Generated Answer: Mirle is a small village located in the Mysore district of Karnataka, India. It's part of the Nanjangud Taluk. The village is known for its serene and peaceful atmosphere, surrounded by lush greenery and agricultural fields. It's a typical example of a rural setting in the southern part of India.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input:  tell my name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Generated Answer: Your name is Santhosh.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input:  break\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Generated Answer: Sure, feel free to reach out whenever you need assistance. Have a great day!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input:  exit\n"
     ]
    }
   ],
   "source": [
    "## Create Chatbot\n",
    "\n",
    "from langchain.schema import SystemMessage, HumanMessage,AIMessage\n",
    "\n",
    "chat_history = [\n",
    "    SystemMessage(content=\"you are a helpful assistant\")\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input=input(\"user_input: \")\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    if user_input==\"exit\":\n",
    "        break\n",
    "    result=llm.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(result.content))\n",
    "    print(\"AI Generated Answer:\", result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873e338-1a5f-466f-ae1b-57193010de95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
